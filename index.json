[
{
	"uri": "/scope/",
	"title": "Scope",
	"tags": [],
	"description": "",
	"content": " We began mapping out the territory of this trade in 2016. We collected thousands of posts and studied the language of the posts - how the collectors and enthusiasts described their engagement with the remains. At that time, we studied only one platform. Our methods were primarily textual.1\nIn this project, we intend to explore the leads suggested in that first study by developing and adapting approaches from machine learning, computer vision, and artificial intelligence (various neural network models) to scale up our ability to study this trade. We are looking at a number of social media platforms and marketplaces.\nBuilding on our previous research, can we marry these insights from machine learning and computer vision, to those generated from text analysis of the posts, and social network analysis of followers? How do particular patterns of display move over the network of participants - are there fads, trends, key players? Finally, what are the ethical, moral, and legal implications of using machine learning in this way?\nOur objectives are therefore:  To develop and share a trained neural network that can be employed by other researchers interested in this trade in particular; To develop the computational and theoretical tools to allow others to adapt our approach to their own area of interest in humanities’ research; To determine the patterns in the visual rhetorics of the trade in human remains online so that this trade can be tracked across social media, monitored, and disrupted; To enable the possibility of sourcing these materials so that they may be repatriated to descendent communities; To build ethical frameworks into our computational approaches; To develop a cohort of highly trained personnel who will take this research forward into other domains.   Huffer, D. and Graham, S. 2017 The Insta-Dead: the rhetoric of the human remains trade on Instagram, Internet Archaeology 45. https://doi.org/10.11141/ia.45.5 [return]   "
},
{
	"uri": "/tutorials/classify-images-with-tensorflow/affinity-propagation-in-r/",
	"title": "Affinity Propagation in R",
	"tags": [],
	"description": "",
	"content": "Introduction Having determined the visual similarity of the vectors, we first reduced the complexity of the resulting data with t-sne, a common first step when working with the output of neural networks. It is similar to principle components analysis, but better suited here because the data has a very high number of dimensions, and so better preserves the patterns.\nIn this notebook, we are then taking this data and exploring it for clusters. We use the ‘Affinity propagation’ technique. From our paper (2018):\n Affinity propagation is a clustering algorithm that identifies exemplars among data points and forms clusters of data points around these exemplars. K-means is often used for clustering but it is sensitive to the initial random selection of exemplars, and does not necessarily select the best representation of clusters; in Frey and Duecke’s approach, all data points are considered as possible exemplars. This approach models the datapoints as a kind of network along which messages are passed recursively. From this, exemplars and clusters emerge (2007). An important difference between k-means and affinity propagation clustering is that for affinity propagation we do not need to specify the number of clusters in advance as we do for k-means\n # import the data library(jsonlite) mydata \u0026lt;- fromJSON(\u0026quot;image_tsne_projections.json\u0026quot;, flatten=TRUE) We can begin by taking a look at the data like so:\n## let\u0026#39;s see what the data looks like ## in this example, we only loaded 25 images (demo) ## so it\u0026#39;s a bit artificial, but carry on. library(ggplot2) g = ggplot(mydata, aes(x = x, y = y)) + geom_point() + labs(title = \u0026quot;T-SNE Plot of Image Similarity Vectors\u0026quot;) print(g) To the eye, there is clearly structure. So how many clusters? We first get the data ready for the affinity propagation algorithm by passing it to a new variable, and then running it through ‘apcluster’:\nX \u0026lt;- cbind(mydata$x, mydata$y) ## affinity propogation library(apcluster) ## ## Attaching package: \u0026#39;apcluster\u0026#39; ## The following object is masked from \u0026#39;package:stats\u0026#39;: ## ## heatmap d.apclus \u0026lt;- apcluster(negDistMat(r=2), X) cat(\u0026quot;affinity propogation optimal number of clusters:\u0026quot;, length(d.apclus@clusters), \u0026quot;\\n\u0026quot;) ## affinity propogation optimal number of clusters: 4 Finally, we can plot these clusters:\nplot(d.apclus, X)  "
},
{
	"uri": "/tutorials/blogdown/",
	"title": "Building a Website with Blogdown",
	"tags": [],
	"description": "A short tutorial on using blogdown to build a website",
	"content": "This website is built with blogdown.\nsetwd(\u0026quot;/Users/path/to/your/files/username.github.io\u0026quot;) blogdown::new_site(dir = \u0026quot;usernameFiles\u0026quot;, #New Directory within working directory where web content will be stored theme = 'vjeantet/hugo-theme-docdock', #theme information format = 'toml') #specify toml instead of yaml setwd(\u0026quot;usernameFiles\u0026quot;) blogdown::build_site() blogdown::serve_site()  That builds the shell of the site in a subfolder inside your username.github.io repository on your computer.\nblogdown::build_site() builds your website. Copy the contents of the generated public folder to the username.github.io directory.\ngit add . git commit -m \u0026quot;first commit\u0026quot; git push -u origin master  The insert image add-in for blogdown makes putting your image in the correct spot a whole lot easier. Make sure to install it. For the full rundown on how to use blogdown, see the manual. et voila.\nWith this particular theme, you generate the navigation structure by nesting folders inside folders and each subfolder has its own _index.md file. In that index file you have weight = 10 or whatever value in the TOML so that folders at the same level of the hierarchy appear in the correct order.\nIn your terminal, navigate to your usernameFiles folder, and run hugo server. Then, any changes you make in rStudio to your source files is automatically reflected at localhost:1313 in your browser.\n"
},
{
	"uri": "/scope/schedule/",
	"title": "Schedule",
	"tags": [],
	"description": "",
	"content": " Years 1 and 2:  Expand and develop a training dataset from the existing corpus of Instagram material, which currently consists of ca. 15 000 photographs, posts, and metadata collected for Huffer and Graham 2017  Identify salient markers upon which to create a supervised learning model Identify appropriate deep-learning unsupervised approaches to create an unsupervised model Use Google’s Inception v3 trained model to identify clusters based on shared image features and compare these results with our own models Evaluate these models against a subset of the original data held in reserve Move forward with the appropriate model(s) (objective 1)   Continue to collect posts and metadata over the duration of the project from the targeted social media platforms Instagram and Facebook to keep the research timely (objective 1)\nSituate our work in broader context of research into the antiquities trade (objectives 3 \u0026amp; 5)\nYears 3 - 5  Develop and train NN targeting different facets of the material: licit versus illicit, sentiments, provenance, demographics to develop a holistic picture (objectives 1, 2, \u0026amp; 3) Develop and release a body of code (under version control) in e.g., Tensorflow and the R statistical computing language, for reproducibility and replicability to other domains of archaeological or cultural materials. Tensorflow is currently the state-of-the-art in exploring NN; the R language and its associated ecosystem of publication workflows has become a standard for digital humanities work. (objectives 1 \u0026amp; 2) Social media platforms evolve quickly. We must continually explore the implications of the changing social media ecosystem, its ‘terms of service’, and evolving thought on the ethics of such research (objective 5)  Years 4 - 5  Push the data and the models further: can we identify likely descendent communities o Associated archival and historical research to support this task (objective 4)  Identify other lines of evidence that support the case for intervention from a legal standpoint (objectives 3, 4, \u0026amp; 5)  Identify the migration of materials across social media platforms by tracking visually similar images (objectives 3, 4, \u0026amp; 5) Develop public-facing tutorials that ethically communicate the results of this research to raise awareness with possible descendent communities, law enforcement, academic and professional audiences, and to advocate for policy changes (objectives 3, 4, \u0026amp; 5)  "
},
{
	"uri": "/tutorials/add-new-file/",
	"title": "Adding New Content Via Github",
	"tags": [],
	"description": "A workflow for writing new content on a gh-pages hosted site",
	"content": " This project website, its public face, is generated using Blogdown. Blogdown itself is built on Hugo, a generator for static websites from simple textfiles. The reason that we use the Blogdown version is that it can handle text written in R-Markdown. And why does R-Markdown matter? This is a version of the markdown text file that can have embedded R code in it, which R can run when the site is generated - and so we can integrate analytical code into our writing.\nRemember when your math teacher said, \u0026lsquo;show your work\u0026rsquo;? Turns out, that is profoundly important for reproducibility and replicability in research. Given that we work at public institutions, using public money, we rather think that\u0026rsquo;s important.\nThe other nice aspect to using simple text files, a site generator, and Github, is that all of our writing and research is kept under version control. Github allows us to edit files directly on their website, which is handy for quick things like a blog post.\nGetting started  Make sure you have a github account. Make sure that you are added as a member of the organization that controls the repo in question - in this case, \u0026lsquo;blog source files\u0026rsquo;. (On Shawn\u0026rsquo;s computer, he\u0026rsquo;s got the \u0026lsquo;content\u0026rsquo; subfolder of the blogdown folder under version control). Organizations you belong to will be shown to the bottom left of the screen when you\u0026rsquo;re on your own Github profile page. Once you\u0026rsquo;re a member, login to Github and navigate directly to the repo. So, make sure you\u0026rsquo;re in blog-source-files. You now have displayed the list of folders from which the content of this site is created.  Writing  Click on the \u0026lsquo;Updates\u0026rsquo; folder. This is the blog portion of this site. Click on \u0026lsquo;create new file\u0026rsquo; You should see the editor, like so:  To make writing a bit more pleasant, tick the \u0026lsquo;no wrap\u0026rsquo; dropdown at the right hand side and select \u0026lsquo;soft wrap\u0026rsquo;.\nGive the blog post a name where the first part is the date, eg: 2018-05-03-learning-a-new-tool.md. The .md part tells us that this is a text file written in the markdown format. Please don\u0026rsquo;t forget that!\nMake sure that the first text you write is the post metadata, which follows the pattern below (this is \u0026lsquo;toml\u0026rsquo; format, by the way):\n+++ title= \u0026quot;Getting Started!\u0026quot; date= 2018-04-16T15:16:31-04:00 description = \u0026quot;getting started\u0026quot; draft= false +++ 2018-04-16 ### Getting Started! We are pleased to announce....  And that\u0026rsquo;s all there is to it. Follow markdown conventions. WARNING Nothing is saved as you type. To save your work, you have to make a \u0026lsquo;commit\u0026rsquo;:\n.\nGive your commit a short descriptive message - this will appear in the list of \u0026lsquo;snapshots\u0026rsquo; that comprise the history of this repository - and add a longer message if you like. Commit directly to the master branch.\nAdding images Any images you want to have appear in the post need to be in this folder as well. In your text, you can add an image using markdown like so: ![image descriptive text](imagename.png) . Then, after you\u0026rsquo;ve committed your text, drag-and-drop your image into the repo (literally drag it onto the list of files in this folder). You\u0026rsquo;ll be asked to make a commit message for this too.\nR markdown Writing an R markdown post works exactly the same way as everything else described above; just make sure that you follow the conventions correctly. Any data files that you want to use in your R markdown should be uploaded to this repository (ie they were in the same working directory when you wrote the R markdown, so they should be in the same folder in the repo when I build the website).\n"
},
{
	"uri": "/papers/",
	"title": "Papers and Publications",
	"tags": [],
	"description": "",
	"content": " Please also see our repositories on Github at https://github.com/bonetrade for work in development. \nJournal Articles 2017 Huffer, D. and Graham, S. 2017 The Insta-Dead: the rhetoric of the human remains trade on Instagram, Internet Archaeology 45. DOI: 10.11141/ia.45.5\nHuffer, D. and Graham, S. 2017 Insta-dead-article Data Repository DOI: 10.5281/zenodo.1040904\n2018 Graham, S. 2018 Fleshing out the bones: data and code repository. Open Science Foundation. DOI 10.17605/OSF.IO/9CFJA\nGraham, S. 2018 Identifying-Similar-Images-with-TensorFlow-notebooks. DOI: 10.5281/zenodo.1243787 | Launch Binder\nHuffer, D. and Graham, S. 2018 Fleshing out the Bones: Studying the Human Remains Trade with Tensorflow and Inception, Journal of Computer Applications in Archaeology 1(1). DOI 10.5334/jcaa.8.\nConference Presentations June 29, 2018. \u0026lsquo;Fleshing out the Bones: understanding the human remains trade with computer vision\u0026rsquo; Transatlantic Cultural Property Crime Symposium - the slides are available here\n"
},
{
	"uri": "/tutorials/",
	"title": "Tutorials",
	"tags": [],
	"description": "Tutorials",
	"content": " How do I do\u0026hellip;.? We will be developing tutorials built around our code. The ideal is that a person following our tutorials will be able to adopt, adapt, and reuse our code towards their own research purposes.\n"
},
{
	"uri": "/scope/bibliography/",
	"title": "Bibliography",
	"tags": [],
	"description": "",
	"content": "Aprile A., Castellano G., Eramo G. 2014. Combining image analysis and modular neural networks for classification of mineral inclusions and pores in archaeological potsherds. Journal of Archaeological Science 50: 262-272.\nBaldwin, B. 1963. Crime and criminals in Graeco-Roman Egypt. Aegyptus 43: 256–263.\nBarceló J.A. 1995. Back-propagation algorithms to computer similarity relationships among archaeological artefacts. In: Wilcock, J. and Lockyear (eds.),Computer Applications andQuantitative Methods in Archaeology 1993. British Archaeological Reports International Series 598, Oxford: Tempus Reparatum, 165-176.\nBarceló, J.A. 2004. Beyond classification: automatic learning in archaeology and cultural studies. Intelligenza Artificiale 1: 29-33.\nBarceló J.A. 2008. Computational Intelligence in Archaeology. Information Science Reference: Hershey, NY.\nBarceló J.A., Faura J.M. 1999. Time series and neural networks in archaeological seriations. An example on early pottery from the Near East. In: Dingwall, L., Exon, S., Gaffney,V., Laflin, S and van Leusen, M. (eds.), Archaeology in the Age of the Internet: CAA97. British Archaeological Reports International Series 750, Oxford:Archaeopress, 91-102.\nBarceló J.A., Pijoan J., Vicente O. 2001. Image quantification as archaeological description. In: Stancic Z., Veljanovski T. (eds.) Computing Archaeology for Understanding the Past. ArchaeoPress: Oxford, pp. 69-78.\nBaxter, M. 2014. Neural networks in archaeology. Essay. https://www.academia.edu/8434624/Neural_networks_in_archaeology\nBeck L. A. (Ed.). 1995. Regional approaches to mortuary analysis. New York: Plenum Publishing Co. Bell S., Croson C. 1998. Artificial neural networks as a tool for archaeological data analysis. Archaeometry 40: 139-151.\nBenhabiles H., Tabia H. 2016. Convolutional neural network for pottery retrieval. Journal of Electronic Imaging 26, doi: http://dx.doi.org/10.1117/1.JEI.26.1.011005.\nBorth D., Ji R., Chen T., Breuel T., Chang S.-F. 2013. Large-scale visual sentiment ontology and detectors using adjective noun pairs. MM \u0026lsquo;13 Proceedings of the 21st ACM international conference on Multimedia, pp. 223-232.\nBrodie N. 2017. The role of conservators in facilitating the theft and trafficking of cultural objects: the case of a seized Libyan statue. Libyan Studies, doi: https://doi.org/10.1017/lis.2017.1.\nCavazos-Rehg P.A., Krauss M.J., Sowles S.J., Bierut L.J. 2016. Marijuana-related posts on Instagram. Prevention Science 17: 710-720.\nChappell D., Huffer D. 2014a. Protecting cultural heritage: A review of some contemporary developments in Australia and near environs. In: Chappell, D., Hufnagel, S. (eds.) Contemporary Perspectives on the Detection, Investigation and Prosecution of Art Crime: Australasian, European and North American Perspectives. 1st Edition. Ashgate Press: London, pp. 237-254.\nChen T., Borth D., Darrell T., Chang S.-F. 2014. DeepSentiBank: Visual sentiment concept classification with deep convolutional neural networks. arXiv:1410.8586.\nChoi C. 2011. NY mummy smugglers reveal vast antiquities black market. http://www. livescience.com/15234-ny-mummy-smugglers-reveal-vast-antiquities-black-market.html.\nCiregan D., Meier U., Schmidhuber J. 2012. Multi-column deep neural networks for image classification. Proceedings of the 2012 IEEE Conference on Computer Vision and Pattern Recognition. DOI: 10.1109/CVPR.2012.6248110.\nContreras D.A. 2010 Huaqueros and remote sensing imagery: assessing looting damage in the Virú Valley, Peru. Antiquity 84: 544-555.\nContreras D.A., Brodie N. 2010. The utility of publicly-available satellite imagery for investigating looting of archaeological sites in Jordan. Journal of Field Archaeology 35: 101-114.\nDavis S. 2015. Meet the living people who collect dead human remains, Vice, http://www.vice.com/read/meet-the-living-people-who-collect-human-remains-713.\nDelicado, P. 1999. Statistics in archaeology: new directions. In Barceló J., Briz, I. and Vila, A.,(eds.), New Techniques for Old Times: CAA98. Oxford: Archaeopress, 29-37.\nDeselaers, T., Pimenidis, L., Ney, H. 2008. Bag-of-visual-words models for adult image classification and filtering. ICPR 2008 19th International Conference on Pattern Recognition DOI: 10.1109/ICPR.2008.4761366.\nEveritt B.S., Dunn G. 2001. Applied Multivariate Data Analysis. 2nd Edition. John Wiley \u0026amp; Sons: Hoboken, NJ.\nFeng Y., Lapata M. 2010. How many words in a picture worth? Automatic caption generation for news images. Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, Uppsala, Sweden, 11-16 July 2010, pg. 1239–1249.\nFerwerda B., Schedl M., Tkalcic M. 2016. Predicting personality traits with Instagram pictures. EMPIRE \u0026lsquo;15 Proceedings of the 3rd Workshop on Emotions and Personality in Personalized Systems 2015, pp. 7- 10.\nFidalgo E., Alegre E., González-Castro V., Fernández-Robles L. 2017. Illegal activity categorization in DarkNet based on image classification using CREIC method. In: Pérez García H., Alfonso-Cendón J., Sánchez González L., Quintián H., Corchado E. (eds.) International Joint Conference SOCO’17- CISIS’17-ICEUTE’17 León, Spain, September 6–8, 2017, Proceeding. SOCO 2017, CISIS 2017, ICEUTE 2017. Advances in Intelligent Systems and Computing, vol 649. Springer, Cham.\nGambino P. (Ed.) 2016. Morbid Curiosities: Collections of the Uncommon and Bizarre. London: Laurence King Publishing.\nGardiner, A. 1997. The Egyptians: An introduction. Oxford: Oxford University Press.\nGearin, C. 2016. Hundreds of mystery human skulls sold on eBay for up to $5500. https://www.newscientist.com/article/2097171-hundreds-of-mystery-human-skulls-sold-on-ebay-for-up- to-5500/.\nGibson P.M. 1996. An archaeofaunal ageing comparative study into the performance of human analysis versus Hybrid Neural Network Analysis. Analecta Praehistorica Leidensia 28-I, 229:233.\nGowland, R., Knüsel, C. (Eds.). 2006. The social archaeology of funerary remains. Oxford: Oxbow Books\nGraham, S. Weingart, S. Millian, I. 2012. Getting Started with topic modeling and MALLET. The Programming Historian. https://programminghistorian.org/lessons/topic-modeling-and-mallet\nGraham S. 2015. Mapping the structure of the archaeological web. Internet Archaeology 39. DOI: 10.11141/ia.39.1\nGraham S., Milligan I., Weingart S. 2015. Exploring big historical data: The historian’s macroscope. Imperial College Press: London.\nGraham S. 2016b. Scraping Instagram with R, with PHP’ Electric Archaeology. https://electricarchaeology.ca/2016/12/06/scraping-instagram-with-r-with-php/\nGoel V., Isaac M. 2016. New Facebook policy bans talk of private gun sales, applies to Instagram, The New York Times, http://www.nytimes.com/2016/01/30/technology/facebook-gun-sales-ban.html?_r=0.\nGovindaraju V., Sher D.B., Srihari R.K., Srihari S.N. 1989. Locating human faces in newspaper photographs. Computer Vision and Pattern Recognition, 1989. Proceedings CVPR \u0026lsquo;89, pp. 549-554.\nGuzzardi N. 2012. New Etsy rules: Bones, hazardous materials and other things you can no longer sell. http://www.huffingtonpost.com.au/entry/new-etsy-rules-no-longer-sell_n_1778310.\nHalling C.L., Seidemann R.M. 2016. They Sell Skulls Online?! A Review of Internet Sales of Human Skulls on eBay and the Laws in Place to Restrict Sales. Journal of Forensic Sciences 61: 1322-1326.\nHaslett C. 2015. Click to like this: Is Instagram a hub for illegal ape deals?, Mongabay, http://news.mongabay.com/2015/12/click-to-like-this-is-instagram-a-hub-for-illegal-ape-deals/.\nHernandez-Castro J., Roberts D.L. 2015. Automatic detection of potentially illegal online sales of elephant ivory via data mining. PeerJ Computer Science 1:e10. Doi: https://doi.org/10.7717/peerj-cs.10.\nHighfield T, Leaver T. 2015. A methodology for mapping Instagram hashtags. First Monday: Peer- Reviewed Journal on the Internet 18. http://firstmonday.org/ojs/index.php/fm/article/view/4711/3698.\nHochman N, Manovich L. 2013. Zooming into an Instagram city: Reading the local through social media. First Monday: Peer-Reviewed Journal on the Internet 20. http://www.firstmonday.dk/ojs/index.php/fm/article/view/5563/4195.\nHosseinmardi H, Mattson SA, Rafiq RI, Han R, Lv Q, Mishra S. 2015. Analyzing labeled cyberbullying incidents on the Instagram social network. In: Liu T.-Y., et al. (eds.) International Conference on Social Informatics. Cham, Switzerland: Springer International Publishing, pp. 49-66.\nHuffer D., Chappell D. 2014a. The mainly nameless and faceless dead: an exploratory study of the illicit traffic in archaeological and ethnographic human remains. Crime, Law, and Social Change 62: 131-153.\nHuffer D., Chappell D. 2014b. Local and international illicit traffic in Vietnamese cultural property: A preliminary investigation. In: Kila J., Balcells M. (Eds.). Cultural Property Crime: An Overview and Analysis of Contemporary Perspectives and Trends. Brill Press: Amsterdam, pp. 263-291.\nHuffer D, Chappell D, Charlton N, Spatola B. in press. Bones of contention: The online trade in archaeological, ethnographic and anatomical human remains on Instagram. In: Chappel D and Hufnagel S (eds.). Art Crime Handbook. London: Palgrave Macmillan Press.\nHuffer D, Graham S. 2017. The Insta-Dead: The rhetoric of the human remains trade on Instagram. Internet Archaeology 45, doi: https://doi.org/10.11141/ia.45.5.\nHugo K. 2016. Human skulls are being sold online, but is it legal? http://news.nationalgeographic.com/2016/08/human-skulls-sale-legal-ebay-forensics-science/.\nHuxley A.K., Finnegan M. 2004. Human remains sold to the highest bidder! A snapshot of the buying and selling of human skeletal remains on eBay®, an internet auction site. Journal of Forensic Science 49: 1-4.\nJuefei-Xu F., Verma E., Goel P., Cherodian A., Savvides M. 2016. DeepGender: Occlusion and low resolution robust facial gender classification via progressively trained convolutional neural networks with attention. The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops, pp. 68-77.\nKalt D. 2016. Why I was wrong about liberal-arts majors. https://blogs.wsj.com/experts/2016/06/01/why-i-was-wrong-about-liberal-arts-majors/.\nKarpathy A. 2015. The unreasonable effectiveness of recurrent neural networks. http://karpathy.github.io/2015/05/21/rnn-effectiveness/.\nKillgrove K. 2016. This archaeologist uses Instagram to track the human skeleton trade. https://www.forbes.com/sites/kristinakillgrove/2016/07/06/this-archaeologist-uses-instagram-to-track- the-human-skeleton-trade/#39cd983f6598.\nKim E. 2012. Etsy blocks sales of drugs and human remains. http://money.cnn.com/2012/08/10/technology/etsy-bans-drugs/index.html\nKinkopf K.M., Beck J. 2016. Bioarchaeological approaches to looting: A case study from Sudan. Journal of Archaeological Science: Reports 10: 263-271.\nKnudson, K. J., Stojanowski, C. M. 2008. New directions in bioarchaeology: recent contributions to the study of human social identities. Journal of Archaeological Research 16: 397–432.\nKohonen T. 1995. Self-organizing maps. Series in information sciences, vol. 30. Springer: Heidelberg.\nKubiczek PA, Mellen PF. 2004. Commentary on: Huxley AK, Finnegan M.; Human remains sold to the highest bidder! A snapshot of the buying and selling of human skeletal remains on eBay, and internet auction site. Journal of Forensic Science 49: 17-20.\nKulkarni G., Premraj V., Ordonez V., Dhar S., Li S., Choi Y., Berg A.C., Berg T.L. 2013. Baby Talk: Understanding and generating simple image descriptions. IEEE Transactions on Pattern Analysis and Machine Intelligence 35, pp. 2891-2903.\nLarson, C. S. 1997. Bioarchaeology: Interpreting behaviour from the human skeleton. Cambridge: Cambridge University Press.\nLasaponara R., Leucci G., Masini N., Persico R. 2014. Investigating archaeological looting using satellite images and GEORADAR: the experience in Lambayeque in North Peru. Journal of Archaeological Science 42: 216-30.\nLi X., Pham T.-A. N., Cong G., Yuan Q., Li X.-L., Krishnaswamy S. 2015. Where you Instagram?: Associating your Instagram photos with points of interest. CIKM \u0026lsquo;15 Proceedings of the 24th ACM International on Conference on Information and Knowledge Management, pp. 1231-1240.\nMcClure T. 2017. Grave robbers with far-right links could be stealing ancestral Maori skulls. https://www.vice.com/en_nz/article/kb4yjw/a-group-of-far-right-grave-robbers-could-be-digging-up- sacred-maori-sites.\nMcNab H. 2016. Macabre collection of human skulls taken from an Indonesian tribe and bound for Australia intercepted by customs. http://www.dailymail.co.uk/news/article-3128950/Macabre-collection- human-skulls-taken-Indonesian-tribe-bound-Australia-intercepted-customs.html.\nMackenzie S., Yates D. 2016a. Trafficking cultural objects and human rights. In: Weber L., Fishwick E., Marmo M. (eds.) The Routledge International Handbook of Criminology and Human Rights. Routledge Press: New York, pp. X-X.\nMackenzie S., Yates D. 2016b. Collectors on illicit collecting: Higher loyalties and other techniques of neutralization in the unlawful collecting of rare and precious orchids and antiquities. Theoretical Criminology 20: 340-357.\nMartin D. L., Harrod R. P. (eds.) 2012. Special forum: new directions in bioarchaeology. SAA Archaeological Record 12: 31.\nMarwick B. 2013. A distant reading of the Day of Archaeology. Github http://github.com/benmarwick/dayofarchaeology\nMarwick B. 2016. Computational Reproducibility in Archaeological Research: Basic Principles and a Case Study of Their Implementation. Journal of Archaeological Method and Theory 24.2: 424–450.\nMarwick B. et al. 2017. Open science in archaeology. SAA Archaeological Record. doi: 10.17605/OSF.IO/3D6XX.\nO’Reilly D. 2007. Shifting trends of heritage destruction in Cambodia: From temples to tombs. Historic Environment 20: 12-16.\nO’Reilly D., von den Driesch A., Voeun V. 2006. Archaeology and archaeozoology of Phum Snay: a late prehistoric cemetery in northwestern Cambodia. Asian Perspectives 45: 188–211.\nRautman A. E. (Ed.). 2000. Reading the body: Representations and remains in the archaeological record. Philadelphia: University of Pennsylvania Press.\nRedman SJ. 2016. Bone rooms: From scientific racism to human prehistory in museums. Harvard University Press: Cambridge, MA.\nReece A.G., Danforth C.M. 2017. Instagram photos reveal predictive markers of depression. EPJ Data Science 6: 15-21.\nSchmidhuber J. 2015. Deep learning in neural networks: An overview. Neural Networks 61: 85-117. Seidemann R.M., Stojanowski C.M. 2009. The identification of a skull recovered from an eBay sale. Journal of Forensic Sciences 54: 1247-1253.\nSmith IV J. 2014. Here\u0026rsquo;s every statistic you could want on Instagram drug dealers, http://observer.com/2014/05/heres-every-statistic-you-could-want-on-instagram-drug-dealers/.\nSpars, S. 2010. Understanding conflict through burial. Neural network analysis of death and burial in the war of 1812. Ontario Archaeology. 89\u0026frasl;90: 58-68.\nThosarat, R. 2001. The destruction of the cultural heritage of Thailand and Cambodia. Trade in Illicit Antiquities: the destruction of the world’s archaeological heritage 1:7-18.\nTotti L.C., Costa F.A., Avila S., Valle E., Meira Jr. W., Almeida V. 2014. The impact of visual attributes on online image diffusion. Proceedings of the 2014 ACM conference on Web science, pp. 42-51.\nTsou M-H, Leitner M. 2013. Visualization of social media: Seeing a mirage or a message? Cartography and Geographic Information Science 40: 55-60.\nUshizima D, Manovich L, Margolis T, Douglass J. 2012. Cultural analytics of large datasets from Flickr. Workshop on Social Media Visualization, Dublin, Ireland, May 20, 2012.\nVergano D. 2016. eBay just nixed its human skull market. https://www.buzzfeed.com/danvergano/skull- sales?utm_term=.gtOnv2yz4#.fbOdBGMzm.\nWang, H. He, Z. Huang, Y., Chen, D., Zhou, Z. 2017. Bodhisattva head images modeling style recognition of Dazu Rock Carvings based on deep convolutional network. Journal of Cultural Heritage 27: 60-71.\nYang X., Luo J. 2017. Tracking illicit drug dealing and abuse on Instagram using multimodal analysis. ACM Transactions on Intelligent Systems and Technology (TIST) - Special Issue: Cyber Security and Regular Papers vol. 8, article no. 58.\nYates D., Mackenzie S., Smith E. 2017. The cultural capitalists: Notes on the ongoing reconfiguration of trafficking culture in Asia. Crime, Media, Culture: An International Journal 13: 245-254.\nYucha JM, Pokines JT, Bartelink EJ. 2017. A comparative taphonomic analysis of 24 trophy skulls from modern forensic cases. Journal of Forensic Sciences 62: 1266-1278.\nZhou X., Yu K., Zhang T., Huang T.S. 2010. Image classificaiton using super-vector coding of local image descriptors. In: Daniilidis K., Maragos P., Paragios N. (eds.) Computer Vision – ECCV 2010. ECCV 2010. Lecture Notes in Computer Science, vol. 6315. Springer, Berlin, Heidelberg.\n"
},
{
	"uri": "/scope/outputs/",
	"title": "Expectations for What We&#39;ll Learn",
	"tags": [],
	"description": "Project outputs",
	"content": "We expect to learn\n the strengths and limitations of neural networks as a lens for studying the trade in human remains - a much finer-grained picture of the scope and size of the trade in human remains the identification of flows of visual signifiers of what is for sale, where it comes from, and where the tastes in this market are set the identification of institutions, collections, cultures, or conflicts from which human skeletal remains actively being circulated are sourced how to deploy neural networks in the service of humanistic and social science inquiry  We expect to provide\n high quality training to exceptional students who will use this training to launch their careers anonymized and ethical exposure of the trade to both the public and to policy makers  As outputs we will have created\n several articles in scholarly and popular journals fully cleaned \u0026amp; anonymized datasets for publication and reuse tutorials for use outside our institutions seven highly trained personnel new course opportunities for undergraduates training and research relationships between Carleton University and the University of Stockholm a new direction for the study of the trade in illicit and illegal antiquities more generally  At the conclusion of this proposed project, we believe that we will\n be in a position to begin another phase of this research where we will take our results and our method, where digital cultural heritage ethics are foregrounded. We hope to be able to support descendent communities at risk, by providing training and support in computational methods.\n be in a position to influence change at social media companies that currently permit the trade in human remains have raised the public consciousness of this trade, including with law-enforcement agencies, policy makers, and the corporations whose platforms currently facilitate the trade  "
},
{
	"uri": "/scope/graduate-opportunities/",
	"title": "Graduate Opportunities",
	"tags": [],
	"description": "",
	"content": " We currently have funded opportunities for PhD and MA students; see below.\nGraduate students in History/Digital Humanities and History/Data Science will be trained in reproducible computational approaches and open science, as well as the necessary grounding in the antiquities trade, and the human remains subtrade. Digital training will include everything from version control to writing analytical packages, to interactive website design for tutorial-writing and outreach. They will learn state of the art machine learning and neural network approaches with social media data.\nStudents will be involved in the outset in planning our research design; they will have opportunities to publish as lead authors (especially with regard to mobilizing knowledge through venues such as The Programming Historian and conferences such as DHSITES). MA students will have the opportunity to work in a leadership role with students in Graham’s undergraduate digital history research methods courses (HIST3812 and HIST3814); the PhD student will have the opportunity to design and teach undergraduate courses on the intersection of data ethics and history/archaeology. The graduate students will be encouraged and supported to hold ‘unconferences’ (see thatcamp.org) on a theme of their choosing; an excellent opportunity exists for international participation and recognition of their work should they choose to hold one (as we will encourage them to do) during the DH2020 Conference in Ottawa.\nTheir research may include:\n Writing code to generate datasets Developing various NN Analyzing results Ground-truthing training datasets (making sure that training images are properly classified) Curating and preparing materials for data publication in appropriate venues Research and writing of tutorials Research and writing connected with their own research interests as they intersect with this project Communicating the results of research with relevant publics at conferences and other venues  Funded PhD Opportunity\n\u0026lsquo;Ethics, Digital Humanities, and the Computer Gaze\u0026rsquo; We are seeking an individual to explore from an ethical perspective for public history and cultural heritage writ large, the use of various digital technologies broadly classified as \u0026lsquo;AI\u0026rsquo; on photographic (or other social media) materials. This research would form an integral part of Shawn Graham and Damien Huffer\u0026rsquo;s \u0026ldquo;The Bone Trade: Studying the Online Trade in Human Remains with Machine Learning and Neural Networks Project\u0026rdquo;. The student would start ideally in September 2018.\nThe candidate\u0026rsquo;s major field would be in Public History, with a breadth field in Digital Humanities\nInterested candidates are invited to contact Dr. Shawn Graham to discuss their potential research project, to gauge their potential fit with the funding envelope and other potential supplementary funding sources. Candidates are also invited to review the Phd Program requirements at https://carleton.ca/history/graduate/phd-program/program-requirements/\nInternational students are also invited to apply.\nCriteria:\n a good MA degree in a relevant subject (history, archaeology, etc) some existing ability in digital humanities methods is desirable  Funded MA Opportunities History, Public History, and Digital Humanities We are seeking two potential MA students (to start September 2018) to collaborate on \u0026ldquo;The Bone Trade: Studying the Online Trade in Human Remains with Machine Learning and Neural Networks Project\u0026rdquo;. The students would pursue their own research within the ambit of this project, which revolves around the use of various AI technologies investigating the trade in human remains online. Ideally, the students\u0026rsquo; own research projects would push the research into other domains, for instance, historical photographs; tourist photos; advertising using historical imagery, digital historical consciousness.\nInterested candidates are invited to contact Dr. Shawn Graham to discuss their potential research project, to gauge their potential fit with the funding envelope and other potential supplementary funding sources. Candidates are also invited to review the MA History, MA Public History, with Collaborative Digital Humanities Program requirements -\nhttps://carleton.ca/history/graduate/ma-program/program-requirements/m-a-with-specialization-in-digital-humanities/\nhttps://carleton.ca/history/m-a-in-public-history/\nCriteria: - a good first degree in a relevant subject (history, archaeology, etc) - existing ability in digital humanities methods or issues is desirable, but not critical. Much more important is an ability to think creatively about the problems or potentials of computational viewpoints.\n"
},
{
	"uri": "/tutorials/scrapy-instagram/",
	"title": "Scraping Instagram Post Metadata",
	"tags": [],
	"description": "A short tutorial on using Scrapy to scrape Instagram",
	"content": " Instagram used to have an API that allowed a very comprehensive exploration of the metadata attached to each . To use this API, one had to register as a developer and agree to the terms of service. Pablo Barbera from the London School of Economics has developed an excellent package for R, instaR to work with the API. However, changes to the API and the approval process in the last few years has restricted use of the public API to mostly commercial purposes. That is to say, they wish to monetize the content. The result is that using Barbera\u0026rsquo;s package is often not feasible for us.\nOther alternatives are possible, and while they do not necessarily provide complete access to the full stream of possible data, they do provide an awful lot. In essence, these approaches involve mimicing an individual paging through search results, very very quickly.\nHere we will get started with a python package written by Github user h4t0n, Andrea Tarquini that depends on the Scrapy framework.\nRequirements:  Python Scrapy  If you are on a mac, you already have python installed. We will install Scrapy inside of a virtual environment, so as to avoid making dependency conflicts and so on with other python packages on our machine.\n$ mkdir scrapytest $ virtualenv scrapytest $ source scrapytest/bin/activate  Install Scrapy: $ pip install scrapy\nDownload the instagram-scraper from the repository.\nUnzip that file. Then, make a new text file at the same level as the instagram-scraper folder. Give that file the extension .cfg\n├── instagram-scraper │ ├── __init__.py │ ├── items.py │ ├── pipelines.py │ ├── settings.py │ └── spiders │ ├── __init__.py │ └── hashtag.py └── scrapy.cfg  Edit that file so that it contains the basic configuration information scrapy needs in order to pull together the scraper that h4t0n made:\n# basic information for scrapy cfg file [settings] default = instagram-scraper.settings [deploy] #url = http://localhost:6800/ project = tutorial  The first thing this file does is tell scrapy where to find the settings.py file in the instagram-scraper folder. The second line isn\u0026rsquo;t even really needed, but these are the defaults when creating a new scrapy scraper, so we\u0026rsquo;ll keep them for posterity.\nRunning the scraper If you look in the instagram-scraper folder, you\u0026rsquo;ll notice the spiders subfolder. The hashtag.py file is the one that lays out how to interact with Instagram. We want Scrapy to use that file, so:\n$ scrapy crawl hashtag\nYou will then be prompted to enter the target hashtag. The spider will then begin searching and scraping. Output prints to the terminal window and to file.\nOutput The output will be in a new folder, \u0026lsquo;scraped\u0026rsquo; and subfolder, \u0026lsquo;hashtag\u0026rsquo;. Each hashtag search will have its own subfolder, and the output file there will have its timestamp as its name. Each line in the output is json formatted. You can concatenate all of the files together with cat * \u0026gt; output.json. To make a properly formatted json file, open output.json in a text editor, add , to each line (except the last one), and put a [ at the start of the file and a ] at the end.\nThe data is now ready to be put into a database for further querying.\nDeactivate your virtualenv:\n$ deactivate\n"
},
{
	"uri": "/tutorials/sqlite/",
	"title": "Setting up a SQLite DB",
	"tags": [],
	"description": "A short tutorial on setting up a sqlite db and simple queries of same",
	"content": " The data that we scraped using Scrapy was output as json files. Json stands for \u0026lsquo;JavaScript Object Notation\u0026rsquo; and records values as attribute-data pairs and lists these pairs in arrays. Because we are not building a complex multi-faceted database, and we might want to expose this data for other kinds of uses later on (perhaps as our own API), we are going to take that data and put it into a sqlite database.\nThis will also enable us to identify duplicate values (data that got captured via multiple searches or hashtags, for instance), extract and reshape the data for particular purposes, connect our data to R for subsequent analyses, and deposit the data in our data repo.\nRequirements  sqlitebiter a python utility for ingesting a wide variety of data formats into an sqlite db python  First, set up a virtualenv:\n$ mkdir database-work $ virtualenv database-work $ source database-work/bin/activate  Install sqlitebiter:\n$ pip install sqlitebiter  This will install a bunch of other dependencies - which is why we\u0026rsquo;re using a virtualenv, so that these dependencies don\u0026rsquo;t conflict with other packages we\u0026rsquo;re using.\nMaking the database Assuming you have the concatenated file containing all of the results from the scrape as a properly formatted json file in the current folder,\nsqlitebiter file scrape-results.json -o scrape-results-current-date.sqlite  Then, to finish things up,\n$ sqlite3 scrape-results-current-date.sqlite \u0026gt;\u0026gt;\u0026gt; .schema \u0026gt;\u0026gt;\u0026gt; .exit  The \u0026gt;\u0026gt;\u0026gt; on the command line show us we\u0026rsquo;re inside sqlite. We could continue to work on the command line with sqlite, but sometimes a gui does make life easier. We\u0026rsquo;ll use sqlitebrowser. Download and install. Then, open the dbbrowser and open the database you just created.\nClick \u0026lsquo;browse data\u0026rsquo; and you\u0026rsquo;ll see the data in a clean table view. The data can be exported as csv. Here are some simple SQL queries we can run to do some common tasks for us. To run these, click \u0026lsquo;execute sql\u0026rsquo; and paste the query into the edit window. Once the query is there (these queries can also be saved and loaded from file) hit the \u0026lsquo;run\u0026rsquo; button to execute.\nExample Queries A query to select distinct rows, that is to say, remove duplicate values:\nselect distinct id, caption, display_url, owner_name, taken_at_timestamp from scrape-results-current-date;  make a new table from those distint values:\ncreate table individual_posts as select distinct id, caption, display_url, owner_name, taken_at_timestamp from scrape-results-current-date;  select distinct rows by keyword in a field:\nselect distinct id, caption, display_url, owner_name, taken_at_timestamp from individual_posts where caption like '%sale%';  convert unix timestamp to human-readable date\nUPDATE unique_posts SET taken_at_timestamp = datetime(taken_at_timestamp, 'unixepoch', 'localtime')  "
},
{
	"uri": "/tutorials/sqlite-to-r/",
	"title": "Importing SQLite DB into R",
	"tags": [],
	"description": "A workflow for importing DB into R",
	"content": " I\u0026rsquo;ve got data, now what? We\u0026rsquo;ve already explained how to put data into a SQLite database. Now we want to grab a subset of that data and pull it into R where we can analayze or visualize it, or combine it with other kinds of data. It is in fact quite straightforward.\nWe write a script that tells R to open a connection to the database; then we tell it what\u0026rsquo;s inside that database; then we write an SQL query in R that grabs just the bits we\u0026rsquo;re interested in and passes it to a new R object; and then we shut the connection and clear the cache.\nIn this way, we\u0026rsquo;re never directly modifying our datastore. We\u0026rsquo;re always working on a copy of the data, which is handy because when we screw up - and we will - we can rest easy knowing the original data is still ok. The code looks like this:\n# first we get the packages we need: library(DBI) library(RSQLite) # now we open the connection: con = dbConnect(SQLite(), dbname=\u0026quot;YOUR-AMAZING-DATABASE.sqlite\u0026quot;) # we can see what's inside, eg, what tables are in the database? alltables = dbListTables(con) alltables # write the query to get the information you want myQuery \u0026lt;- dbSendQuery(con, \u0026quot;SELECT id, owner_id, caption, taken_at_timestamp FROM unique_posts\u0026quot;) # pass that information to an R object. The n = -1 bit means grab everything until there's nothing left to grab. Otherwise, you can specify how many rows etc. my_data \u0026lt;- dbFetch(myQuery, n = -1) # now that we're done, clear cache dbClearResult(myQuery) # now carry on and begin manipulating my_data # for more information see # http://tiffanytimbers.com/querying-sqlite-databases-from-r/ # also perhaps this https://www.r-bloggers.com/using-sqlite-in-r/  "
},
{
	"uri": "/tutorials/classify-images-with-tensorflow/",
	"title": "Cluster Images with Tensorflow",
	"tags": [],
	"description": "",
	"content": "In our 2018 paper, \u0026lsquo;Fleshing out the Bones\u0026rsquo;, we used Duhaime\u0026rsquo;s modified classify_images.py script using Tensorflow and Inception 3 to extract the image vectors from the second-to-last layer of the cnn (before labelling). These image vectors can then be fed through cluster_vectors.py to find nearest neighbors. This data can be visualized with t-sne; then we use affinity propagation to see the clusters in R.\nYou may obtain and run the code for yourself in our online jupyter notebook binder.\nThe repository for this data is at https://osf.io/9cfja/.\n"
},
{
	"uri": "/tutorials/tensorflow-for-poets/",
	"title": "Build an Image Classifier with Tensorflow",
	"tags": [],
	"description": "Grab data and build an image classifier",
	"content": " To build an image classifier, we will follow the Tensorflow for Poets tutorial. While the tutorial is more or less straightforward, there are still some hidden gotchas.\nGet some data But first, we need to have data arranged into labelled categories, like so (imagining we were working with Roman fabrics and wares):\n| |-training-images | |-terrasig |-african_red_slip |-veranice_nera  etc. Potsherd.net has a lot of this information, and it\u0026rsquo;s well organized. For this tutorial, we\u0026rsquo;ll use that data as our own data is not yet ready for this stage.\nThe first thing to do is to write a scraper to grab the images and put them in sensible folders.\nScrapy can do this. But I haven\u0026rsquo;t really learned how to use Scrapy from scratch yet - it\u0026rsquo;s one thing to repurpose or use code that someone else has developed, quite another to write ex novo. The Programming Historian has an excellent piece by Jeri Wieringa on using \u0026lsquo;Beautiful Soup\u0026rsquo; to parse a webpage - we\u0026rsquo;ll use that in conjunction with wget to get the information we want.\nPotbot.py Open a text editor and start a new file. First thing we\u0026rsquo;ll do is import some modules that make life easier:\nfrom bs4 import BeautifulSoup import csv import requests r = requests.get('http://potsherd.net/atlas/Ware/') soup = BeautifulSoup(r.content, \u0026quot;html.parser\u0026quot;)  We create a variable r that tells python we want it to go out onto the web and retrieve the html at that location; we then tell python to pass the html to BeautifulSoup for processing. Next, we tell the program to create a new csv file for us to use as our output container:\nf = csv.writer(open(\u0026quot;output.csv\u0026quot;, \u0026quot;w\u0026quot;)) f.writerow([\u0026quot;Domain\u0026quot;, \u0026quot;Path\u0026quot;]) # Write column headers as the first line  Then comes the magic. We tell python, via BeautifulSoup, to look in the html for a table (having studied the html of our target site, we know that the links to the individual ware pages are organized using html table tags) and then grab the information contained in the a tags.\ntrs = soup.find_all('tr') for tr in trs: for link in tr.find_all('a'): fulllink = link.get ('href') print (fulllink) #print in terminal to verify results f.writerow([\u0026quot;http://potsherd.net/atlas/\u0026quot;, fulllink]) # Write column headers as the first line  et voila. Save all that as a .py file, and from the terminal, run it:\n$ python potbot.py\nA new file \u0026lsquo;output.csv\u0026rsquo; is created with two columns. The first column contains http://potsherd.net/atlas/ and the second column contains the rest of the paths we want. Delete all , and delete the header row; save as urls.txt and we can then use wget like so:\nwget -r -w 2 --limit-rate=20k -A jpeg,jpg,bmp,gif,png -i urls.txt\nThis command tells wget to follow the paths in the input file urls.txt, to wait a moment between requests, to limit the amount asked for each time, and to only keep images. Go get a coffee, come back later, and you\u0026rsquo;ll have a lovely directory of data.\nThe Tensorflow bit Set up a virtual environment first - that is to say, a special arrangement where you are using python and installing bits and pieces just for this tutorial, just to this particular version of python. Here is a primer on how to do this. If you don\u0026rsquo;t use a virtual environment, you probably won\u0026rsquo;t notice any problems - at first. But as you do more and more code work, you\u0026rsquo;ll start encountering conflicts and weird errors as various packages are installed and fight with each other.\nMy virtualenv uses python 3.6.1.\nGet tensorflow:\n$ pip install --upgrade \u0026quot;tensorflow==1.7.*\u0026quot;  Get the Tensorflow for poets codebase:\n$ git clone https://github.com/googlecodelabs/tensorflow-for-poets-2 $ cd tensorflow-for-poets-2  Look at that directory in your finder. There\u0026rsquo;s a folder called tf_files. Copy the data you want to use as training data into that folder; if you\u0026rsquo;re using the pottery data from above, copy the gallery folder so that you now have tf_files/gallery. (nb make sure you have the right gallery folder - potsherd.net has two folders in slightly different places called gallery. One has image data arranged by ware; one doesn\u0026rsquo;t.)\nThe tutorial now wants you to set some environment variables, and to launch a process to monitor everything. You don\u0026rsquo;t need to do this. If you do decide to launch the process, eg,\n$ tensorboard --logdir tf_files/training_summaries \u0026amp;  \u0026hellip;you\u0026rsquo;ll need to open a new tab in your terminal and activate your virtual environment again (eg., $source env/bin/activate) to carry on. Having this monitoring process running also lets you use a browser-based tool at http://0.0.0.0:6006/ to do some other high-level tensor things, but these aren\u0026rsquo;t necessary for our purposes today.\nMoving on - it\u0026rsquo;s time to retrain the model! The command, in the tutorial, looks like this:\npython -m scripts.retrain \\ --bottleneck_dir=tf_files/bottlenecks \\ --how_many_training_steps=500 \\ --model_dir=tf_files/models/ \\ --summaries_dir=tf_files/training_summaries/\u0026quot;${ARCHITECTURE}\u0026quot; \\ --output_graph=tf_files/retrained_graph.pb \\ --output_labels=tf_files/retrained_labels.txt \\ --architecture=\u0026quot;${ARCHITECTURE}\u0026quot; \\ --image_dir=tf_files/flower_photos  See that ${ARCHITECTURE]} bit? That can be set using an environment variable. For our purposes, for the time being, we can just put the various possibilities in ourselves. I found this website useful in that regard. Because our pottery training data - though copious - is still not enough (fewer than 10000 images), we have to add another flag to our command, concerning validation batch size. Otherwise we\u0026rsquo;ll get an error message. Note also that we\u0026rsquo;re only training for 500 steps; more steps will generally get better results (but diminishing returns also apply).\nOur command then:\npython -m scripts.retrain \\ --bottleneck_dir=tf_files/bottlenecks \\ --how_many_training_steps=500 \\ --model_dir=tf_files/models/ \\ --summaries_dir=tf_files/training_summaries/mobilenet_0.50_224 \\ --output_graph=tf_files/retrained_graph.pb \\ --output_labels=tf_files/retrained_labels.txt \\ --architecture mobilenet_0.50_224 \\ --validation_batch_size=-1 \\ --image_dir=tf_files/gallery  This will run for a while. Elements to explore: different mobilenet architectures, increasing iterations, more data. Once it\u0026rsquo;s finished training, let\u0026rsquo;s test it:\npython -m scripts.label_image \\ --graph=tf_files/retrained_graph.pb \\ --image=tf_files/gallery/B4/B4-1-f.jpg  So we\u0026rsquo;re testing it on one of the images we trained it with. Our result:\nEvaluation time (1-image): 0.265s b4 (score=0.96954) c189 (score=0.00678) dr2 4 (score=0.00401) oxrs (score=0.00397) gaul (score=0.00185)  Try a different image:\npython -m scripts.label_image \\ --graph=tf_files/retrained_graph.pb \\ --image=tf_files/gallery/SEGL/SEGL-ext1-part-1.jpg  Another very high result. But the proof is in the pudding- what kind of results do we get on new, never-seen-before images? A quick google search for SEGL brought up this article: https://phys.org/news/2017-06-rare-archaeological-unique-pottery-south.html which has an image of some pottery that, per the article, is SEGL. Right-click, save-as. I put it in a new folder, tf_files/testing.\npython -m scripts.label_image \\ --graph=tf_files/retrained_graph.pb \\ --image=tf_files/testing/rarearchaeol.jpg  Result:\nEvaluation time (1-image): 0.268s oxrs (score=0.53120) hars (score=0.07437) c189 (score=0.06991) como (score=0.04226) vrw (score=0.03977)  So\u0026hellip; it doesn\u0026rsquo;t agree with the article, but it\u0026rsquo;s not very sure itself. This is a function of not enough training data I suspect (one could interpret it as the people in the article being wrong about the attribution of the pottery, and of course, that raises the interesting philosophical condundrum of whether the machine is a ware lumper or a splitter).\nConclusion In this tutorial, we\u0026rsquo;ve learned how to scrape a website for data, and to build an image classifier from that data. Much fine tuning and better data are required to build something where the results will be meaningful and useful. It should be apparent to the reader that much depends on just how the parameters are tuned, and the kind, quality, and quantity of training images used.\n"
},
{
	"uri": "/scope/end-of-project/",
	"title": "When This Project Ends",
	"tags": [],
	"description": "How this site and project data shall be archived",
	"content": "When this project reaches its conclusion, this website will no longer be updated. It will remain online here on Github for as long as Github allows us to host it here. We will also feed the url into the Internet Archive periodically to enable snapshots of the site and project there. We will create a web archive file for the site (for more on web archives, see the Web Archive Research Group).\nOur code, cleaned/anonymized data, articles, and the materials on this site will be deposited in Zenodo.org with appropriate DOIs.\nEach journal article will have its own research compendium attached, to promote replicability and reproducibility of our research (see Marwick et al 2017, \u0026lsquo;Open Science and Archaeology\u0026rsquo; SAA Archaeological Record)\nOur shared Zotero library may be viewed at https://www.zotero.org/groups/2174147/bonetrade-project.\n"
},
{
	"uri": "/update/2018-07-29-dcgan-for-archaeologists/",
	"title": "DCGAN for Archaeologists",
	"tags": [],
	"description": "learning how to build a dcgan",
	"content": " 2018-07-29\nLearning about GANs I had a very interesting conversation with Melvin Wevers, who has been using neural networks to understand visual patterns in the evolution of newspaper advertisements in Holland. He and his team developed a tool for visually searching the newspaper corpus. Melvin presented some of his research at #dh2018; he shared his poster and slides so I was able to have a look. Afterwards, I reached out to Melvin and we had a long conversation about using computer vision in historical research.\nHis poster is called \u0026lsquo;ImageTexts: Studying Images and Texts in Conjunction\u0026rsquo; which clearly is relevant to our work in the BoneTrade. In his research, he looks at the text for \u0026lsquo;bursty\u0026rsquo; changes in the composition of the text. That is, points where the content changes \u0026lsquo;state\u0026rsquo; in terms of the frequency of the word distribution. The other approach is to use Generative Adversarial Networks on the images.\nSo what are GAN? This post is a nice introduction and uses this image to capture the idea:\nIn essence, you have two networks. One learning how to identify your source images, and the second learning how to fool the first by creating new images from scratch.\nWhy should we care about this sort of thing? For our purposes here, it is one way of learning just what features of our source images our identifiers are looking for (there are others of course). Remember that one of the points of our research is to understand the visual rhetoric of these images. If we can successfully trick the network, then we know what aspects of the network we should be paying attention to. Another intriguing aspect of this approach is that it allows a kind of \u0026lsquo;semantic arithmetic\u0026rsquo; of the kind we\u0026rsquo;re familiar with from word vectors:\n The easiest way to think about words and how they can be added and subtracted like vectors is with an example. The most famous is the following: king – man + woman = queen. In other words, adding the vectors associated with the words king and woman while subtracting man is equal to the vector associated with queen. This describes a gender relationship.\nAnother example is: paris – france + poland = warsaw. In this case, the vector difference between paris and france captures the concept of capital city.\n  MIT Tech Review, Sept 17 2015  I will admit that I haven\u0026rsquo;t figured out quite how to do this yet, but I\u0026rsquo;ve found various code snippets that should permit this.\nFinally, as Wevers puts it, \u0026lsquo;The verisimilitude of the generated images is an indication of the meaningfulness of the learned subspace\u0026rsquo;. That is, if our generated images are not much good, then that\u0026rsquo;s an indication that there\u0026rsquo;s just far too much noise going on in our source data in the first place. Garbage in, garbage out. In Melvin\u0026rsquo;s poster, the GAN \u0026ldquo;was able to learn the variances in car models, styling, color, position and photographic composition seen in the adverts themselves.\u0026rdquo;\nIn which case, it seems that GANS are a worthwhile avenue to explore for our research.\nDominic Monn published an article and accompanying Jupyter Notebook on building a GAN trained on one of the standard databases, \u0026lsquo;CelebFaces Attributes data set\u0026rsquo; which has more than 200,000 photographs of \u0026lsquo;celebrities\u0026rsquo; (training dataset composition is a topic for another post). It\u0026rsquo;s probably a function of my computer but I couldn\u0026rsquo;t get this up and running correctly (setting up and using AWS computing power will be a post and tutorial in due course). It is interesting in that it does walk you through the code, which is not as forbidding as I\u0026rsquo;d initially assumed.\nI had more success with Taehoon Kim\u0026rsquo;s \u0026lsquo;tensorflow implementation of \u0026ldquo;Deep Convolutional Generative Adversarial Networks\u0026rdquo;\u0026rsquo;, which is available on Github at https://github.com/carpedm20/DCGAN-tensorflow. I don\u0026rsquo;t have a GPU on this particular machine, so everything was running via CPU; I had to leave my machine for a day or two, and also use the caffeinate command on my Mac to keep it from going to sleep while the process ran (quick info on this here).\nI had a number of false starts. Chief amongst these was the composition of my training set.\n You need lots of images. Reading around, 10 000 seems to be a bottom minimum for meaningful results The images need to be thematically unified somehow. You can\u0026rsquo;t just dump everything you\u0026rsquo;ve got. I went through a recent scrape of instagram via the tag skullforsale and pulled out about 2300 skull images. That was enough to get the code to run, but as you\u0026rsquo;ll see, not the best results. Of course, I was only trying to learn how to use the code and work out what the hidden gotchas were.  Gotchas Ah yes, the gotchas.\n images have to be small. Resize them to 256 x 256 or 64 x 64 pixels. Use Imagemagick\u0026rsquo;s \u0026lsquo;mogrify\u0026rsquo; command. images have to be rgb weird errors about casting into array eg https://github.com/carpedm20/DCGAN-tensorflow/issues/162: ValueError: could not broadcast input array from shape (128,128,3) into shape (128,128) means that we have to use Imagemagick\u0026rsquo;s \u0026lsquo;convert\u0026rsquo; command there too. greyscale images screw things up. Convert those to RGB as well running the code: use the dockerized version, and put the data inside the DATA folder. running the code: python main.py --dataset=skulls --data_dir data --train --crop The \u0026ndash;crop always has to be there.  convert image1.jpg -colorspace sRGB -type truecolor image1.jpg  make sure there are no grayscale images\nidentify -format \u0026quot;%i %[colorspace]\\n\u0026quot; *.jpg | grep -v sRGB  convert images to 64x64\nmogrify -resize 64x64 *.jpg  convert to sRGB\nmogrify -colorspace sRGB *.jpg  Results? I let the code run until it reached the end of its default iteration time (which is a function of the size of your images). Results were\u0026hellip; unimpressive. With too small a dataset, the code would simply not run. With my 2300 images, I did get things that hint at something better\u0026hellip;\nAfter nearly an hour the first visualization of the results after a mere two epochs of iterations\u0026hellip; a dreamy mist-scape as the machine creates.\nIn this mosaic, which represents the results from my first actual working run (20 epochs), you can, if you squint, see a nightmarish vision of monstrous skulls. Too few images, I thought (about a thousand, at this point). So I spent several hours collecting more images, and tried again\u0026hellip;\nMaybe I\u0026rsquo;m only seeing what I want to see, but I see hints of the orbital bones around the eyes, the bridge of the nose, in the top left side of each test image in the mosaic.\nSo. I think this approach could prove productive, but I need a) more computing power and b) more images. I wonder if I can remove my decision making process in the creation of the corpus from this process. Could I construct a pipeline that feeds the mass of images we\u0026rsquo;ve created into a CNN, use the penultimate layer and some clustering to create various folders of similar images, and then pass the folders to the GAN to figure out what it\u0026rsquo;s looking at, and visualize the individual neurons?\n"
},
{
	"uri": "/update/2018-06-14-tensorflow-for-poets/",
	"title": "Tensorflow for Poets",
	"tags": [],
	"description": "building a classifier with tensorflow",
	"content": "2018-06-14\nIn our recent paper, \u0026lsquo;Fleshing Out the Bones\u0026rsquo; we used the trained \u0026lsquo;Inception 3\u0026rsquo; model as a way of determining clusters of images that we then studied for clues and hints: why did the machine cluster them this way? What are the common features?\nUnsupervised learning: It\u0026rsquo;s not unlike reading entrails.\nAn alternate approach is to take an existing model, and add new training data to it. Pete Warden put together a tutorial a few years ago called Tensorflow for Poets that has since been formalized as a Google CodeLabs tutorial. Last night I tried the tutorial out using a corpus of Roman fabrics and wares. The nuts-and-bolts of doing this are over in the tutorials.\nThe hardest part was getting the training data organized. It needs to be in a folder where each image is in a subfolder where the name of the subfolder is the category, eg:\n| |-training-images | |-terrasig |-african_red_slip |-veranice_nera  \u0026hellip;etc. Once that was done, it went quite smoothly. What will take some time is figuring out what the different architecture and other flags do. For instance, in the default command suggested by the tutorial, I had to determine that I needed to add the flag on validation size and set it to use the entire training set (as my training set is probably way too small).\npython -m scripts.retrain \\ --bottleneck_dir=tf_files/bottlenecks \\ --how_many_training_steps=500 \\ --model_dir=tf_files/models/ \\ --summaries_dir=tf_files/training_summaries/mobilenet_0.50_224 \\ --output_graph=tf_files/retrained_graph.pb \\ --output_labels=tf_files/retrained_labels.txt \\ --architecture mobilenet_0.50_224 \\ --validation_batch_size=-1 \\ --image_dir=tf_files/gallery   \u0026ldquo;The architecture flag is where we tell the retraining script which version of MobileNet we want to use. The 1.0 corresponds to the width multiplier, and can be 1.0, 0.75, 0.50 or 0.25. The 224 corresponds to image resolution, and can be 224, 192, 160 or 128. For example, to train the smallest version, you’d use \u0026ndash;architecture mobilenet_0.25_128.\u0026rdquo; Harvey, 2017\n I\u0026rsquo;m not sure I understand exactly what this all means yet, practically speaking. Anyway, I now know what I\u0026rsquo;ll need our MA research assistants to do this fall.\n study our first corpus\u0026rsquo; clustering results to come up with some training categories divide that corpus into a training dataset train a new image classifier run that classifier on our new corpus (which I\u0026rsquo;m still collecting) compare that with the clustering approach without our new classifier. compare the results of both with the posts\u0026rsquo; text  The ambition? To be able to sort at scale the images we find automatically into sensible structure.\n"
},
{
	"uri": "/update/2018-04-30-using-etudier-to-jump-start-a-literature-review/",
	"title": "Using Etudier to Jump Start a Literature Review",
	"tags": [],
	"description": "A workflow for starting that literature review",
	"content": " 2018-04-30\nThe Literature Review How do you start a literature review on a new project? We\u0026rsquo;re not starting from scratch, of course - we\u0026rsquo;ve got the pages of bibliography that we generated when we first starting scratching out what this project could be about. We started that step by considering the bibliography we had to hand as we wrote our first paper. Nothing is ever truly from scratch, in academia.\nBut we can certainly jump start the process. One way we can do that is by pulling from one of the largest bibliographic databases readily to hand - Google Scholar. G-Scholar was built by ingesting several existing databases and is maintained by whatever magic Google uses to index journal websites and so on. We can search this database not only how we might search with the regular Google search page, but also via the citation graph. We can give it a reference, and search not only what that paper cites, but also the papers that cited it in turn. What\u0026rsquo;s more, we can automate this process.\nEd Summers has created a python package called \u0026lsquo;Etudier\u0026rsquo; which handles this for us. It mimics a user paging through search results. By default it only looks at the first ten citations for a page or a search result, and then the top 10 for each of those, thus 100 results. That might be enough to get going. But by fiddling with the --pages and --depth flags, you can collect much much more. pages will search through x number of pages of results, while depth will delve that far into each result\u0026rsquo;s citations.\nI ran the following:\n$etudier.py 'https://scholar.google.com/scholar?hl=en\u0026amp;as_sdt=0%2C21\u0026amp;q=\u0026quot;instagram\u0026quot;\u0026amp;btnG=' --pages 2 --depth 2  \u0026hellip;to see what the broader landscape of scholarship around Instagram looked like. This produced 1.4 mb of results, or 1883 articles tied by 2179 edges.\nAs a graph, we spot something interesting right away:\nTwo broad clumps, or two extremely distinct ways of referring to \u0026lsquo;Instagram\u0026rsquo;. We can then explore subclumping by searching for communities or patterns of link similarities (aka as \u0026lsquo;modularity\u0026rsquo;):\nOften however this kind of visualization is not of much use, other than how we\u0026rsquo;ve already used it. Instead, let\u0026rsquo;s look for centrality in this graph, and identify those works whose pattern of linkages enable them to bridge these various subgroups. I presume that such works have something about them that speaks to these different aspects of scholarship and so are the works that I\u0026rsquo;ll want to start with for my review. Using gephi I calculate eigenvector centrality (roughly, the centrality that comes from being wellconnected wellconnected others).\nYou can download our graph of data for yourself here.\n"
},
{
	"uri": "/update/2018-04-16-getting-started/",
	"title": "Getting Started!",
	"tags": [],
	"description": "getting started",
	"content": " 2018-04-16\nWe\u0026rsquo;re underway! We received word at the end of March that this project has been funded. Since then, we\u0026rsquo;ve been doing the administrative work to get the project up and running - figuring out how to advertise for students, how to enable those students to study at Stockholm for a term or two, setting up the public face of this project, figuring out the internals of the research accounting system.\nWe also took advantage of the experience of writing this proposal to run a small experiment in using Tensorflow on our previous corpus (Huffer \u0026amp; Graham 2017) of Instagram materials. We wrote that up for the Journal of Computer Applications in Archaeology and submitted it around the same time as we submitted the proposal. This was accepted with revisions, and is now in the pipeline. We hope to see it come out shortly. That paper was also the nucleus for a paper at the Society for American Archaeology conference this April in Washington DC. We also presented a related paper, drawing on Damien\u0026rsquo;s postdoctoral work- Huffer and Graham, Bioarchaeological Approaches to Investigating Supply, Demand and Authenticity in the Colonial-era Human Remains Trade.\nI guess this all means: we\u0026rsquo;re hitting the ground running!\n"
},
{
	"uri": "/",
	"title": "The Bonetrade: Studying the Online Trade in Human Remains",
	"tags": [],
	"description": "The online trade in human remains is little explored. We use neural network approaches to try to understand the visual and textual rhetorics that underpin it.",
	"content": " The Bonetrade: Studying the Online Trade in Human Remains with Machine Learning and Neural Networks There is a thriving online trade in anatomical, ethnographic and archaeological human remains that makes ready use of new social media such as Instagram, Facebook, Etsy, and until recently, eBay. The \u0026ldquo;fetishization\u0026rdquo; of the \u0026lsquo;exotic\u0026rsquo; dead that underpins this trade by its very nature transforms pieces of the body into material culture: curios, commodities or objets d\u0026rsquo;art. This practice has deep Colonial-era roots, but today\u0026rsquo;s e-commerce and social media platforms have only expanded collectors\u0026rsquo; reach and made participation open to anyone with interest and spare finances. The sheer volume of materials being produced, shared, and sold can be overwhelming for a small team to study. The market moves so fast.\nCan we teach machines to identify from photographs alone patterns in the \u0026lsquo;visual rhetoric\u0026rsquo; that signal materials for sale? Can \u0026lsquo;licit\u0026rsquo; materials be discerned from \u0026lsquo;illicit\u0026rsquo;? Are there geographical patterns? Can we trace materials back to a source?\nThis website is the public face of our research. Here you will find updates, code, research compendia, papers, presentations, and other elements of our work over the next five years. We intend to produce a body of data and of code that will enable other researchers to repurpose our research into other allied fields (such as the more familiar trade in archaeological antiquities, and in the markets for other \u0026lsquo;grey market\u0026rsquo; and black market commodities).\nOpen Research All our code and data relating to this project are publicly available. All our publications and reports are accompanied by a compendium of code and data files that are deposited in a trustworthy data repository and referenced via DOI. We use GitHub to publicly host our code in development, at github.com/bonetrade. We archive code and data files at the Open Science Foundation.\nCitation Graham S. and Huffer D. 2018. The Bonetrade Project Website - bonetrade.github.io. Zenodo. DOI: 10.5281/zenodo.1245091\nGraham S. and Huffer D. 2018. The Bonetrade Project Source Text Files. Zenodo. DOI: 10.5281/zenodo.1241500\n"
},
{
	"uri": "/papers/cultpropcrime-06-18/",
	"title": "Slides: Cultural Property Crime Seminar",
	"tags": [],
	"description": "",
	"content": "  fleshing out the bones\nunderstanding the human remains trade\nwith computer vision    Shawn Graham \u0026amp; Damien Huffer\n@electricarchaeo @DamienHuffer follow along at j.mp/sg-dh-qm18 Note: - late 19th century \u0026amp; scientific racism - human zoos \u0026amp; the human cost\nAbraham Ulrikab Note: - Abraham Ulrikab - Moravian Christians in Labrador. Convinved to come to Europe to be part of a human zoo, because Abraham trusted, wanted to see Europe. Realized immediately the mistake. Kept a diary, translation of which was recently surfaced. Skulls and bones of him and his family found in museum collections in Paris \u0026amp; Berlin. Process of repatriation has begun\nHow many other Abrahams are out there? Note: - every skull, every femur being traded was once a person. This trade dehumanizes, fetishizes, \u0026lsquo;others\u0026rsquo;, these people. Digital colonialism, revisits violence\nOur project:  seeks to map this trade uses tools of digital humanities to understand both text and visual materials  when people want to buy bones, where do they go?\nwhen people want to sell bones, where do they go?\nNote:\ngiven that much of this trade is morally dubious and legally in various grey zones\nhow do they negotiate this sale?\n \u0026lsquo;I have a pile of teeny human skull scraps laying around. Due to etsys rules i cannot sell human bone or make a listing but id love to do custom orders for anyone interested in a pendant, ring, etc made from a human skull fragment. Dm me! #bone #bones #skull #humanbone #humanskull #fragment #skullfragment #oddities #oddity.\u0026rsquo; https://www.instagram.com/p/znHyR7AbXS/\n Note: Not everyone is as bold as this. Most are aware that their posts can be searched (ie key-word searching), and so rely on more subtle cues\nOn Instagram alone where dollar values were named\n total amounted to approximately $190,000\n the value of private sales cannot of course be known\n Cultural impact: no.s of followers, following of just those accounts that name a price\n a network with 138,014 individuals connected by 172,208 links\n  association with taxidermy, tattoo parlours - do i still have that cardiff photo? three broad groupings: specialists explicitly interested in bones; generalists who follow the bones but also other things; and people who don\u0026rsquo;t post pictures themselves but follow the bone accounts\nNote: video of man in Belgium degreasing a skull. Likely source for such a skull? WW1 battlefield. He\u0026rsquo;s preparing it for cutting into an exploded view.\ndesecration of ww2 battleship wrecks - steel is pre 1945, so different atomic signature thus valuable; bones collected, given to the boss\u0026hellip; what happens to them?\nHow do we deal with it?\nPart of the problem is that not all of this is illegal. approaches and issues are of a part with broader trade in antiquities. Also: human remains are not property under common law!\nbroader trade tends to focus on the auction houses and the big collectors; smaller objects, ebay, etc do not attract as much attention\nbut the techniques developed to deal with these smaller items could be of significance for studying the broader trade, and the penetration of archaeological consciousness into the broader publics\ndigital archaeology meets digital humanities Note:\ndigital humanities approaches to data mining try to understand the contextual significance of what has been extracted\nin our work, we have in the first instance scraped ~ 13 000 posts from one calendar year on Instagram (one of many places where human remains are traded); some posts were caught in the trawl as far back as 2013\nwe focussed on the language the posts, building topic models and word vectors to understand some of the rhetorical constructions of buying and selling human Remains\nNote: Exploring the vector space defined by binary pairs, \u0026lsquo;good\u0026rsquo;-\u0026lsquo;bad\u0026rsquo;, \u0026lsquo;for sale\u0026rsquo;-\u0026lsquo;not for sale’. Hints of various dynamics at play can be viewed in contrasting, but related, terms, e.g. as with ‘acrylic painting’ falling in negative space and ‘skull decor’ falling in positive space. This might be a glimpse into the taste or aesthetic of those who create skull-related art, which may be important for understanding what creates the taste and demand for human remains themselves.\nliteral posts, \u0026lsquo;human skulls for sale\u0026rsquo; do exist; but often the language is coded, more oblique, and seems to depend on the composition of the image itself to signal something for sale\nso that\u0026rsquo;s what we\u0026rsquo;ve done here\u0026rsquo;s where computer vision comes into it Note: our current research is looking at the composition of the images themselves, computationally. There are too many images to do the classificatory work ourselves. Some approaches to these kind of image classification or determinations of similarity employ \u0026lsquo;human\u0026rsquo; computers like the Amazon Mechanical Turk service. There are serious ethical issues with employing that kind of labour on this material.\n- Yannick Asogba, \u0026lsquo;Machine Visions\u0026lsquo;\n- Yannick Asogba, \u0026lsquo;Machine Visions\u0026lsquo;\n- Tanz \u0026amp; Carter 2017\n- Yale DH lab, code repo\nNote: if however we stop the process just before the image labelling, we can use the vector representations of our images to compute similarities (the full details will be in our forthcoming JCAA paper). We are completely indebted to Douglas Duhaime and colleagues for clearly explicating and coding how this can be done.\nNote:\ndisclaimer: These images are deliberately distant so that we can discuss the structure of what we\u0026rsquo;re finding but not put the dead on display.\nThis is where we are at right now. After turning all of the images into vector representations, we use some statistical magic (t-sne and affinity clustering propagation) to understand the structure of visual similarity in the posts. The clustering algorithm identifies \u0026lsquo;exemplar\u0026rsquo; images for each of the clusters that it finds.\nNote:\nWe examine these exemplars - in our corpus, that\u0026rsquo;s 84 images - by eye to work out what it is that the computer is \u0026lsquo;seeing\u0026rsquo;, comparing and correlating with our data mining of the text of posts themselves..\nNote:\nSo far, it appears that the computer can discriminate skulls for sale by their arrangement on the shelf. Items for sale are arranged in ways that mimic \u0026lsquo;popular\u0026rsquo; understandings of museology - glass cabinets with skulls arranged by size for instance.\nOther clusters have been photographed square to the face, and largely fill the frame, and the associated language is largely of \u0026lsquo;look at my collection\u0026rsquo; and \u0026lsquo;look what i just gave away\u0026rsquo;, perhaps signaling in other images that these materials could also be \u0026lsquo;given away\u0026rsquo; - for a price. Another cluster positions the skulls such that they are turned slightly to the left; associated texts here clearly indicate something for sale.\nThis initial experiment does seem to support the idea that items for sale are displayed in ways that are discernible to the machine, and so, the machine can be taught to trawl other bodies of data for more evidence of the trade in human remains. The machine directs our attention to the framing of photographs, and the relationship of the human remains to other elements within the photograph. Exhibition design – rows of objects in cases on display – are recreated here. The interplay of foreground and background also seems to be important. Photos composed to show off a collection might also be subtly signaling that the item might also be for sale. These signals could be isolated, and used to train further iterations of a CNN, allowing a researcher to scale up their investigation. We intend to cross-reference this data with the network of followers and followed, to see how these visual clusters play out across networks of influence and on other platforms aside from Instagram.\nTraining our own classifier Build Your Own Classifier Tutorial\nFind a path through image-space Note: While this is an art-history application, I think we might be able to use a similar approach to find pathways of influence through images, if we start with images where we know that a followed/follower relationship exists. Matthew Lincoln, art historian \u0026amp; data scientist at the Getty\n Ryan Baumann Finding Near-Matches in the Rijksmuseum with Pastec  Note: Another way to find or determine influence- pastec looks for near copies, rather than visually similar. Which images are reposted, which occur across social networks? CNN could find this too, but this lets us zero in more quickly \u0026ldquo;Pastec is an open source index and search engine for image recognition.\u0026rdquo; \u0026lt;- something like this might be very useful when trying to match eg polaroids to auction catalogues\nEthically Troubling   ![](faceall1.png) ![](faceall2.png)   It\u0026rsquo;s still early days. Note:\nfor previous slide Our funders are interested in us being able to identify descendent communities for the purposes of repatriation. Can we really do this? And if we are able, our experience in building classifiers so far just points out that what we believe about the images is easy to encode: if we say these are \u0026lsquo;mowhawk\u0026rsquo;, they become reified as mohawk. Finding visual tropes is one thing; saying that, beneath the tropes, this skull was a member of group x, y, z is I think a dangerous line to pursue. But is the good of repatriation worth the risk?\nfor current slide This work is in its early stages. Despite being trained on a very generic corpus of images, Inception v3 seems to be a very powerful model for pulling out some of the visual rhetoric of display. If we train a model ourselves explicitly on archaeological materials, can we identify automatically at scale legal from illegal sales? ethical from non-ethical displays of remains? can we build an archaeo-crawler that would search these images, posts, and peoples out? would such a use be ethical? could one work out likely source peoples to whom these remains belong? given the use of computer vision for surveillance and social control, to what degree is our work dangerous and complicit? How do we ethically use computation to combat the trade?\nthank you shawn.graham@carleton.ca\nThis research has been generously supported by the Social Sciences and Humanities Research Council of Canada\nbonetrade.github.io for technical details, see our paper in The Journal of Computer Applications in Archaeology, DOI: 10.5334/jcaa.8\nImage Credits  dramatization of Abraham Ulrikab - screenshot from CBC The Nature of Things, Trapped in a Human Zoo screenshot of museum exhibiting skeletons - screenshot from CBC The Nature of Things, Trapped in a Human Zoo Tanz\u0026amp; Carter, GumGum Neural Networks Made Easy sources for Instagram screenshots will not be shared in public  Credits for Code \u0026amp; Demos \u0026amp; Screenshots  Finding Near-Matches in the Rijksmuseum with Pastec Ryan Baumann Identifying Similar Images with Tensorflow Douglas Duhaime Mechanical Kubler Matthew Lincoln Machine Visions Yannick Asogba PixPlot visualization Yale DH Lab Tensorflow for Poets Peter Warden/Google Codelabs  "
},
{
	"uri": "/_footer/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "This research was supported by the Social Sciences and Humanities Research Council of Canada\n"
},
{
	"uri": "/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/credits/",
	"title": "Credits",
	"tags": [],
	"description": "contributors and packages used by hugo-theme-docdock",
	"content": " Principle Investigators Shawn Graham Graham is an archaeologist who is currently Associate Professor of Digital Humanities in the History Department at Carleton University. With Ian Milligan and Scott Weingart, he is the author of ‘Exploring Historical Big Data: The \u0026lsquo;Historian’s Macroscope’ (2015). He has written a number of tutorials for The Programming Historian and his research blog (e.g., 2016b) on different computational techniques for both analysis and representation of historical data. He is founder and editor of Epoiesen: A Journal for Creative Engagement in History and Archaeology and is currently building the Open Digital Archaeology Textbook (or ODATE), which will serve as a living resource of how to do digital archaeology in a way that “encourages innovative, reflective, and critical use of open access data and the development of digital tools that facilitate linkages and analysis across varied digital sources”.\nDamien Huffer Huffer is a human bioarchaeologist and osteologist by training. His postdoctoral work incorporates the collection of photographic and osteological data from ‘trophy skull’ assemblages (primarily Bornean Dayak and W. Papuan peoples) held in Swedish, US, and UK museums. He is a recognized authority on the osteological aspects of the trade in human remains.\nGraduate Students This project will feature prominently the work of our graduate students.\nUndergraduate Students Undergraduate students will also have the opportunity to work on aspects of this project as part of their course work, and will be recognized here.\nColophon This website is built using the docdocks theme for Hugo and is powered by Yihui Xie\u0026rsquo;s blogdown environment in R Studio\n"
},
{
	"uri": "/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/update/",
	"title": "Updates",
	"tags": [],
	"description": "",
	"content": ""
}]