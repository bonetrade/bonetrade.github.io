[
{
	"uri": "/scope/",
	"title": "Scope",
	"tags": [],
	"description": "",
	"content": " We began mapping out the territory of this trade in 2016. We collected thousands of posts and studied the language of the posts - how the collectors and enthusiasts described their engagement with the remains. At that time, we studied only one platform. Our methods were primarily textual.1\nIn this project, we intend to explore the leads suggested in that first study by developing and adapting approaches from machine learning, computer vision, and artificial intelligence (various neural network models) to scale up our ability to study this trade. We are looking at a number of social media platforms and marketplaces.\nBuilding on our previous research, can we marry these insights from machine learning and computer vision, to those generated from text analysis of the posts, and social network analysis of followers? How do particular patterns of display move over the network of participants - are there fads, trends, key players? Finally, what are the ethical, moral, and legal implications of using machine learning in this way?\nOur objectives are therefore:  To develop and share a trained neural network that can be employed by other researchers interested in this trade in particular; To develop the computational and theoretical tools to allow others to adapt our approach to their own area of interest in humanities’ research; To determine the patterns in the visual rhetorics of the trade in human remains online so that this trade can be tracked across social media, monitored, and disrupted; To enable the possibility of sourcing these materials so that they may be repatriated to descendent communities; To build ethical frameworks into our computational approaches; To develop a cohort of highly trained personnel who will take this research forward into other domains.   Huffer, D. and Graham, S. 2017 The Insta-Dead: the rhetoric of the human remains trade on Instagram, Internet Archaeology 45. https://doi.org/10.11141/ia.45.5 [return]   "
},
{
	"uri": "/tutorials/classify-images-with-tensorflow/affinity-propagation-in-r/",
	"title": "Affinity Propagation in R",
	"tags": [],
	"description": "",
	"content": "Introduction Having determined the visual similarity of the vectors, we first reduced the complexity of the resulting data with t-sne, a common first step when working with the output of neural networks. It is similar to principle components analysis, but better suited here because the data has a very high number of dimensions, and so better preserves the patterns.\nIn this notebook, we are then taking this data and exploring it for clusters. We use the ‘Affinity propagation’ technique. From our paper (2018):\n Affinity propagation is a clustering algorithm that identifies exemplars among data points and forms clusters of data points around these exemplars. K-means is often used for clustering but it is sensitive to the initial random selection of exemplars, and does not necessarily select the best representation of clusters; in Frey and Duecke’s approach, all data points are considered as possible exemplars. This approach models the datapoints as a kind of network along which messages are passed recursively. From this, exemplars and clusters emerge (2007). An important difference between k-means and affinity propagation clustering is that for affinity propagation we do not need to specify the number of clusters in advance as we do for k-means\n # import the data library(jsonlite) mydata \u0026lt;- fromJSON(\u0026quot;image_tsne_projections.json\u0026quot;, flatten=TRUE) We can begin by taking a look at the data like so:\n## let\u0026#39;s see what the data looks like ## in this example, we only loaded 25 images (demo) ## so it\u0026#39;s a bit artificial, but carry on. library(ggplot2) g = ggplot(mydata, aes(x = x, y = y)) + geom_point() + labs(title = \u0026quot;T-SNE Plot of Image Similarity Vectors\u0026quot;) print(g) To the eye, there is clearly structure. So how many clusters? We first get the data ready for the affinity propagation algorithm by passing it to a new variable, and then running it through ‘apcluster’:\nX \u0026lt;- cbind(mydata$x, mydata$y) ## affinity propogation library(apcluster) ## ## Attaching package: \u0026#39;apcluster\u0026#39; ## The following object is masked from \u0026#39;package:stats\u0026#39;: ## ## heatmap d.apclus \u0026lt;- apcluster(negDistMat(r=2), X) cat(\u0026quot;affinity propogation optimal number of clusters:\u0026quot;, length(d.apclus@clusters), \u0026quot;\\n\u0026quot;) ## affinity propogation optimal number of clusters: 4 Finally, we can plot these clusters:\nplot(d.apclus, X)  "
},
{
	"uri": "/tutorials/blogdown/",
	"title": "Building a Website with Blogdown",
	"tags": [],
	"description": "A short tutorial on using blogdown to build a website",
	"content": "This website is built with blogdown.\nsetwd(\u0026quot;/Users/path/to/your/files/username.github.io\u0026quot;) blogdown::new_site(dir = \u0026quot;usernameFiles\u0026quot;, #New Directory within working directory where web content will be stored theme = 'vjeantet/hugo-theme-docdock', #theme information format = 'toml') #specify toml instead of yaml setwd(\u0026quot;usernameFiles\u0026quot;) blogdown::build_site() blogdown::serve_site()  That builds the shell of the site in a subfolder inside your username.github.io repository on your computer.\nblogdown::build_site() builds your website. Copy the contents of the generated public folder to the username.github.io directory.\ngit add . git commit -m \u0026quot;first commit\u0026quot; git push -u origin master  The insert image add-in for blogdown makes putting your image in the correct spot a whole lot easier. Make sure to install it. For the full rundown on how to use blogdown, see the manual. et voila.\nWith this particular theme, you generate the navigation structure by nesting folders inside folders and each subfolder has its own _index.md file. In that index file you have weight = 10 or whatever value in the TOML so that folders at the same level of the hierarchy appear in the correct order.\nIn your terminal, navigate to your usernameFiles folder, and run hugo server. Then, any changes you make in rStudio to your source files is automatically reflected at localhost:1313 in your browser.\n"
},
{
	"uri": "/scope/schedule/",
	"title": "Schedule",
	"tags": [],
	"description": "",
	"content": " Years 1 and 2:  Expand and develop a training dataset from the existing corpus of Instagram material, which currently consists of ca. 15 000 photographs, posts, and metadata collected for Huffer and Graham 2017  Identify salient markers upon which to create a supervised learning model Identify appropriate deep-learning unsupervised approaches to create an unsupervised model Use Google’s Inception v3 trained model to identify clusters based on shared image features and compare these results with our own models Evaluate these models against a subset of the original data held in reserve Move forward with the appropriate model(s) (objective 1)   Continue to collect posts and metadata over the duration of the project from the targeted social media platforms Instagram and Facebook to keep the research timely (objective 1)\nSituate our work in broader context of research into the antiquities trade (objectives 3 \u0026amp; 5)\nYears 3 - 5  Develop and train NN targeting different facets of the material: licit versus illicit, sentiments, provenance, demographics to develop a holistic picture (objectives 1, 2, \u0026amp; 3) Develop and release a body of code (under version control) in e.g., Tensorflow and the R statistical computing language, for reproducibility and replicability to other domains of archaeological or cultural materials. Tensorflow is currently the state-of-the-art in exploring NN; the R language and its associated ecosystem of publication workflows has become a standard for digital humanities work. (objectives 1 \u0026amp; 2) Social media platforms evolve quickly. We must continually explore the implications of the changing social media ecosystem, its ‘terms of service’, and evolving thought on the ethics of such research (objective 5)  Years 4 - 5  Push the data and the models further: can we identify likely descendent communities o Associated archival and historical research to support this task (objective 4)  Identify other lines of evidence that support the case for intervention from a legal standpoint (objectives 3, 4, \u0026amp; 5)  Identify the migration of materials across social media platforms by tracking visually similar images (objectives 3, 4, \u0026amp; 5) Develop public-facing tutorials that ethically communicate the results of this research to raise awareness with possible descendent communities, law enforcement, academic and professional audiences, and to advocate for policy changes (objectives 3, 4, \u0026amp; 5)  "
},
{
	"uri": "/tutorials/add-new-file/",
	"title": "Adding New Content Via Github",
	"tags": [],
	"description": "A workflow for writing new content on a gh-pages hosted site",
	"content": " This project website, its public face, is generated using Blogdown. Blogdown itself is built on Hugo, a generator for static websites from simple textfiles. The reason that we use the Blogdown version is that it can handle text written in R-Markdown. And why does R-Markdown matter? This is a version of the markdown text file that can have embedded R code in it, which R can run when the site is generated - and so we can integrate analytical code into our writing.\nRemember when your math teacher said, \u0026lsquo;show your work\u0026rsquo;? Turns out, that is profoundly important for reproducibility and replicability in research. Given that we work at public institutions, using public money, we rather think that\u0026rsquo;s important.\nThe other nice aspect to using simple text files, a site generator, and Github, is that all of our writing and research is kept under version control. Github allows us to edit files directly on their website, which is handy for quick things like a blog post.\nGetting started  Make sure you have a github account. Make sure that you are added as a member of the organization that controls the repo in question - in this case, \u0026lsquo;blog source files\u0026rsquo;. (On Shawn\u0026rsquo;s computer, he\u0026rsquo;s got the \u0026lsquo;content\u0026rsquo; subfolder of the blogdown folder under version control). Organizations you belong to will be shown to the bottom left of the screen when you\u0026rsquo;re on your own Github profile page. Once you\u0026rsquo;re a member, login to Github and navigate directly to the repo. So, make sure you\u0026rsquo;re in blog-source-files. You now have displayed the list of folders from which the content of this site is created.  Writing  Click on the \u0026lsquo;Updates\u0026rsquo; folder. This is the blog portion of this site. Click on \u0026lsquo;create new file\u0026rsquo; You should see the editor, like so:  To make writing a bit more pleasant, tick the \u0026lsquo;no wrap\u0026rsquo; dropdown at the right hand side and select \u0026lsquo;soft wrap\u0026rsquo;.\nGive the blog post a name where the first part is the date, eg: 2018-05-03-learning-a-new-tool.md. The .md part tells us that this is a text file written in the markdown format. Please don\u0026rsquo;t forget that!\nMake sure that the first text you write is the post metadata, which follows the pattern below (this is \u0026lsquo;toml\u0026rsquo; format, by the way):\n+++ title= \u0026quot;Getting Started!\u0026quot; date= 2018-04-16T15:16:31-04:00 description = \u0026quot;getting started\u0026quot; draft= false +++ 2018-04-16 ### Getting Started! We are pleased to announce....  And that\u0026rsquo;s all there is to it. Follow markdown conventions. WARNING Nothing is saved as you type. To save your work, you have to make a \u0026lsquo;commit\u0026rsquo;:\n.\nGive your commit a short descriptive message - this will appear in the list of \u0026lsquo;snapshots\u0026rsquo; that comprise the history of this repository - and add a longer message if you like. Commit directly to the master branch.\nAdding images Any images you want to have appear in the post need to be in this folder as well. In your text, you can add an image using markdown like so: ![image descriptive text](imagename.png) . Then, after you\u0026rsquo;ve committed your text, drag-and-drop your image into the repo (literally drag it onto the list of files in this folder). You\u0026rsquo;ll be asked to make a commit message for this too.\nR markdown Writing an R markdown post works exactly the same way as everything else described above; just make sure that you follow the conventions correctly. Any data files that you want to use in your R markdown should be uploaded to this repository (ie they were in the same working directory when you wrote the R markdown, so they should be in the same folder in the repo when I build the website).\n"
},
{
	"uri": "/papers/",
	"title": "Papers and Publications",
	"tags": [],
	"description": "",
	"content": " Please also see our repositories on Github at https://github.com/bonetrade for work in development. \nJournal Articles 2017 Huffer, D. and Graham, S. 2017 The Insta-Dead: the rhetoric of the human remains trade on Instagram, Internet Archaeology 45. DOI: 10.11141/ia.45.5\nHuffer, D. and Graham, S. 2017 Insta-dead-article Data Repository DOI: 10.5281/zenodo.1040904\n2018 Graham, S. 2018 Fleshing out the bones: data and code repository. Open Science Foundation. DOI 10.17605/OSF.IO/9CFJA\nGraham, S. 2018 Identifying-Similar-Images-with-TensorFlow-notebooks. DOI: 10.5281/zenodo.1243787 | Launch Binder\nHuffer, D. and Graham, S. 2018 Fleshing out the Bones: Studying the Human Remains Trade with Tensorflow and Inception, Journal of Computer Applications in Archaeology. In Press.\nConference Presentations Many of our slidedecks for conference presentations are built with reveal.js. They will also be availabel here in due course! "
},
{
	"uri": "/tutorials/",
	"title": "Tutorials",
	"tags": [],
	"description": "Tutorials",
	"content": " How do I do\u0026hellip;.? We will be developing tutorials built around our code. The ideal is that a person following our tutorials will be able to adopt, adapt, and reuse our code towards their own research purposes.\n"
},
{
	"uri": "/scope/bibliography/",
	"title": "Bibliography",
	"tags": [],
	"description": "",
	"content": "Aprile A., Castellano G., Eramo G. 2014. Combining image analysis and modular neural networks for classification of mineral inclusions and pores in archaeological potsherds. Journal of Archaeological Science 50: 262-272.\nBaldwin, B. 1963. Crime and criminals in Graeco-Roman Egypt. Aegyptus 43: 256–263.\nBarceló J.A. 1995. Back-propagation algorithms to computer similarity relationships among archaeological artefacts. In: Wilcock, J. and Lockyear (eds.),Computer Applications andQuantitative Methods in Archaeology 1993. British Archaeological Reports International Series 598, Oxford: Tempus Reparatum, 165-176.\nBarceló, J.A. 2004. Beyond classification: automatic learning in archaeology and cultural studies. Intelligenza Artificiale 1: 29-33.\nBarceló J.A. 2008. Computational Intelligence in Archaeology. Information Science Reference: Hershey, NY.\nBarceló J.A., Faura J.M. 1999. Time series and neural networks in archaeological seriations. An example on early pottery from the Near East. In: Dingwall, L., Exon, S., Gaffney,V., Laflin, S and van Leusen, M. (eds.), Archaeology in the Age of the Internet: CAA97. British Archaeological Reports International Series 750, Oxford:Archaeopress, 91-102.\nBarceló J.A., Pijoan J., Vicente O. 2001. Image quantification as archaeological description. In: Stancic Z., Veljanovski T. (eds.) Computing Archaeology for Understanding the Past. ArchaeoPress: Oxford, pp. 69-78.\nBaxter, M. 2014. Neural networks in archaeology. Essay. https://www.academia.edu/8434624/Neural_networks_in_archaeology\nBeck L. A. (Ed.). 1995. Regional approaches to mortuary analysis. New York: Plenum Publishing Co. Bell S., Croson C. 1998. Artificial neural networks as a tool for archaeological data analysis. Archaeometry 40: 139-151.\nBenhabiles H., Tabia H. 2016. Convolutional neural network for pottery retrieval. Journal of Electronic Imaging 26, doi: http://dx.doi.org/10.1117/1.JEI.26.1.011005.\nBorth D., Ji R., Chen T., Breuel T., Chang S.-F. 2013. Large-scale visual sentiment ontology and detectors using adjective noun pairs. MM \u0026lsquo;13 Proceedings of the 21st ACM international conference on Multimedia, pp. 223-232.\nBrodie N. 2017. The role of conservators in facilitating the theft and trafficking of cultural objects: the case of a seized Libyan statue. Libyan Studies, doi: https://doi.org/10.1017/lis.2017.1.\nCavazos-Rehg P.A., Krauss M.J., Sowles S.J., Bierut L.J. 2016. Marijuana-related posts on Instagram. Prevention Science 17: 710-720.\nChappell D., Huffer D. 2014a. Protecting cultural heritage: A review of some contemporary developments in Australia and near environs. In: Chappell, D., Hufnagel, S. (eds.) Contemporary Perspectives on the Detection, Investigation and Prosecution of Art Crime: Australasian, European and North American Perspectives. 1st Edition. Ashgate Press: London, pp. 237-254.\nChen T., Borth D., Darrell T., Chang S.-F. 2014. DeepSentiBank: Visual sentiment concept classification with deep convolutional neural networks. arXiv:1410.8586.\nChoi C. 2011. NY mummy smugglers reveal vast antiquities black market. http://www. livescience.com/15234-ny-mummy-smugglers-reveal-vast-antiquities-black-market.html.\nCiregan D., Meier U., Schmidhuber J. 2012. Multi-column deep neural networks for image classification. Proceedings of the 2012 IEEE Conference on Computer Vision and Pattern Recognition. DOI: 10.1109/CVPR.2012.6248110.\nContreras D.A. 2010 Huaqueros and remote sensing imagery: assessing looting damage in the Virú Valley, Peru. Antiquity 84: 544-555.\nContreras D.A., Brodie N. 2010. The utility of publicly-available satellite imagery for investigating looting of archaeological sites in Jordan. Journal of Field Archaeology 35: 101-114.\nDavis S. 2015. Meet the living people who collect dead human remains, Vice, http://www.vice.com/read/meet-the-living-people-who-collect-human-remains-713.\nDelicado, P. 1999. Statistics in archaeology: new directions. In Barceló J., Briz, I. and Vila, A.,(eds.), New Techniques for Old Times: CAA98. Oxford: Archaeopress, 29-37.\nDeselaers, T., Pimenidis, L., Ney, H. 2008. Bag-of-visual-words models for adult image classification and filtering. ICPR 2008 19th International Conference on Pattern Recognition DOI: 10.1109/ICPR.2008.4761366.\nEveritt B.S., Dunn G. 2001. Applied Multivariate Data Analysis. 2nd Edition. John Wiley \u0026amp; Sons: Hoboken, NJ.\nFeng Y., Lapata M. 2010. How many words in a picture worth? Automatic caption generation for news images. Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, Uppsala, Sweden, 11-16 July 2010, pg. 1239–1249.\nFerwerda B., Schedl M., Tkalcic M. 2016. Predicting personality traits with Instagram pictures. EMPIRE \u0026lsquo;15 Proceedings of the 3rd Workshop on Emotions and Personality in Personalized Systems 2015, pp. 7- 10.\nFidalgo E., Alegre E., González-Castro V., Fernández-Robles L. 2017. Illegal activity categorization in DarkNet based on image classification using CREIC method. In: Pérez García H., Alfonso-Cendón J., Sánchez González L., Quintián H., Corchado E. (eds.) International Joint Conference SOCO’17- CISIS’17-ICEUTE’17 León, Spain, September 6–8, 2017, Proceeding. SOCO 2017, CISIS 2017, ICEUTE 2017. Advances in Intelligent Systems and Computing, vol 649. Springer, Cham.\nGambino P. (Ed.) 2016. Morbid Curiosities: Collections of the Uncommon and Bizarre. London: Laurence King Publishing.\nGardiner, A. 1997. The Egyptians: An introduction. Oxford: Oxford University Press.\nGearin, C. 2016. Hundreds of mystery human skulls sold on eBay for up to $5500. https://www.newscientist.com/article/2097171-hundreds-of-mystery-human-skulls-sold-on-ebay-for-up- to-5500/.\nGibson P.M. 1996. An archaeofaunal ageing comparative study into the performance of human analysis versus Hybrid Neural Network Analysis. Analecta Praehistorica Leidensia 28-I, 229:233.\nGowland, R., Knüsel, C. (Eds.). 2006. The social archaeology of funerary remains. Oxford: Oxbow Books\nGraham, S. Weingart, S. Millian, I. 2012. Getting Started with topic modeling and MALLET. The Programming Historian. https://programminghistorian.org/lessons/topic-modeling-and-mallet\nGraham S. 2015. Mapping the structure of the archaeological web. Internet Archaeology 39. DOI: 10.11141/ia.39.1\nGraham S., Milligan I., Weingart S. 2015. Exploring big historical data: The historian’s macroscope. Imperial College Press: London.\nGraham S. 2016b. Scraping Instagram with R, with PHP’ Electric Archaeology. https://electricarchaeology.ca/2016/12/06/scraping-instagram-with-r-with-php/\nGoel V., Isaac M. 2016. New Facebook policy bans talk of private gun sales, applies to Instagram, The New York Times, http://www.nytimes.com/2016/01/30/technology/facebook-gun-sales-ban.html?_r=0.\nGovindaraju V., Sher D.B., Srihari R.K., Srihari S.N. 1989. Locating human faces in newspaper photographs. Computer Vision and Pattern Recognition, 1989. Proceedings CVPR \u0026lsquo;89, pp. 549-554.\nGuzzardi N. 2012. New Etsy rules: Bones, hazardous materials and other things you can no longer sell. http://www.huffingtonpost.com.au/entry/new-etsy-rules-no-longer-sell_n_1778310.\nHalling C.L., Seidemann R.M. 2016. They Sell Skulls Online?! A Review of Internet Sales of Human Skulls on eBay and the Laws in Place to Restrict Sales. Journal of Forensic Sciences 61: 1322-1326.\nHaslett C. 2015. Click to like this: Is Instagram a hub for illegal ape deals?, Mongabay, http://news.mongabay.com/2015/12/click-to-like-this-is-instagram-a-hub-for-illegal-ape-deals/.\nHernandez-Castro J., Roberts D.L. 2015. Automatic detection of potentially illegal online sales of elephant ivory via data mining. PeerJ Computer Science 1:e10. Doi: https://doi.org/10.7717/peerj-cs.10.\nHighfield T, Leaver T. 2015. A methodology for mapping Instagram hashtags. First Monday: Peer- Reviewed Journal on the Internet 18. http://firstmonday.org/ojs/index.php/fm/article/view/4711/3698.\nHochman N, Manovich L. 2013. Zooming into an Instagram city: Reading the local through social media. First Monday: Peer-Reviewed Journal on the Internet 20. http://www.firstmonday.dk/ojs/index.php/fm/article/view/5563/4195.\nHosseinmardi H, Mattson SA, Rafiq RI, Han R, Lv Q, Mishra S. 2015. Analyzing labeled cyberbullying incidents on the Instagram social network. In: Liu T.-Y., et al. (eds.) International Conference on Social Informatics. Cham, Switzerland: Springer International Publishing, pp. 49-66.\nHuffer D., Chappell D. 2014a. The mainly nameless and faceless dead: an exploratory study of the illicit traffic in archaeological and ethnographic human remains. Crime, Law, and Social Change 62: 131-153.\nHuffer D., Chappell D. 2014b. Local and international illicit traffic in Vietnamese cultural property: A preliminary investigation. In: Kila J., Balcells M. (Eds.). Cultural Property Crime: An Overview and Analysis of Contemporary Perspectives and Trends. Brill Press: Amsterdam, pp. 263-291.\nHuffer D, Chappell D, Charlton N, Spatola B. in press. Bones of contention: The online trade in archaeological, ethnographic and anatomical human remains on Instagram. In: Chappel D and Hufnagel S (eds.). Art Crime Handbook. London: Palgrave Macmillan Press.\nHuffer D, Graham S. 2017. The Insta-Dead: The rhetoric of the human remains trade on Instagram. Internet Archaeology 45, doi: https://doi.org/10.11141/ia.45.5.\nHugo K. 2016. Human skulls are being sold online, but is it legal? http://news.nationalgeographic.com/2016/08/human-skulls-sale-legal-ebay-forensics-science/.\nHuxley A.K., Finnegan M. 2004. Human remains sold to the highest bidder! A snapshot of the buying and selling of human skeletal remains on eBay®, an internet auction site. Journal of Forensic Science 49: 1-4.\nJuefei-Xu F., Verma E., Goel P., Cherodian A., Savvides M. 2016. DeepGender: Occlusion and low resolution robust facial gender classification via progressively trained convolutional neural networks with attention. The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops, pp. 68-77.\nKalt D. 2016. Why I was wrong about liberal-arts majors. https://blogs.wsj.com/experts/2016/06/01/why-i-was-wrong-about-liberal-arts-majors/.\nKarpathy A. 2015. The unreasonable effectiveness of recurrent neural networks. http://karpathy.github.io/2015/05/21/rnn-effectiveness/.\nKillgrove K. 2016. This archaeologist uses Instagram to track the human skeleton trade. https://www.forbes.com/sites/kristinakillgrove/2016/07/06/this-archaeologist-uses-instagram-to-track- the-human-skeleton-trade/#39cd983f6598.\nKim E. 2012. Etsy blocks sales of drugs and human remains. http://money.cnn.com/2012/08/10/technology/etsy-bans-drugs/index.html\nKinkopf K.M., Beck J. 2016. Bioarchaeological approaches to looting: A case study from Sudan. Journal of Archaeological Science: Reports 10: 263-271.\nKnudson, K. J., Stojanowski, C. M. 2008. New directions in bioarchaeology: recent contributions to the study of human social identities. Journal of Archaeological Research 16: 397–432.\nKohonen T. 1995. Self-organizing maps. Series in information sciences, vol. 30. Springer: Heidelberg.\nKubiczek PA, Mellen PF. 2004. Commentary on: Huxley AK, Finnegan M.; Human remains sold to the highest bidder! A snapshot of the buying and selling of human skeletal remains on eBay, and internet auction site. Journal of Forensic Science 49: 17-20.\nKulkarni G., Premraj V., Ordonez V., Dhar S., Li S., Choi Y., Berg A.C., Berg T.L. 2013. Baby Talk: Understanding and generating simple image descriptions. IEEE Transactions on Pattern Analysis and Machine Intelligence 35, pp. 2891-2903.\nLarson, C. S. 1997. Bioarchaeology: Interpreting behaviour from the human skeleton. Cambridge: Cambridge University Press.\nLasaponara R., Leucci G., Masini N., Persico R. 2014. Investigating archaeological looting using satellite images and GEORADAR: the experience in Lambayeque in North Peru. Journal of Archaeological Science 42: 216-30.\nLi X., Pham T.-A. N., Cong G., Yuan Q., Li X.-L., Krishnaswamy S. 2015. Where you Instagram?: Associating your Instagram photos with points of interest. CIKM \u0026lsquo;15 Proceedings of the 24th ACM International on Conference on Information and Knowledge Management, pp. 1231-1240.\nMcClure T. 2017. Grave robbers with far-right links could be stealing ancestral Maori skulls. https://www.vice.com/en_nz/article/kb4yjw/a-group-of-far-right-grave-robbers-could-be-digging-up- sacred-maori-sites.\nMcNab H. 2016. Macabre collection of human skulls taken from an Indonesian tribe and bound for Australia intercepted by customs. http://www.dailymail.co.uk/news/article-3128950/Macabre-collection- human-skulls-taken-Indonesian-tribe-bound-Australia-intercepted-customs.html.\nMackenzie S., Yates D. 2016a. Trafficking cultural objects and human rights. In: Weber L., Fishwick E., Marmo M. (eds.) The Routledge International Handbook of Criminology and Human Rights. Routledge Press: New York, pp. X-X.\nMackenzie S., Yates D. 2016b. Collectors on illicit collecting: Higher loyalties and other techniques of neutralization in the unlawful collecting of rare and precious orchids and antiquities. Theoretical Criminology 20: 340-357.\nMartin D. L., Harrod R. P. (eds.) 2012. Special forum: new directions in bioarchaeology. SAA Archaeological Record 12: 31.\nMarwick B. 2013. A distant reading of the Day of Archaeology. Github http://github.com/benmarwick/dayofarchaeology\nMarwick B. 2016. Computational Reproducibility in Archaeological Research: Basic Principles and a Case Study of Their Implementation. Journal of Archaeological Method and Theory 24.2: 424–450.\nMarwick B. et al. 2017. Open science in archaeology. SAA Archaeological Record. doi: 10.17605/OSF.IO/3D6XX.\nO’Reilly D. 2007. Shifting trends of heritage destruction in Cambodia: From temples to tombs. Historic Environment 20: 12-16.\nO’Reilly D., von den Driesch A., Voeun V. 2006. Archaeology and archaeozoology of Phum Snay: a late prehistoric cemetery in northwestern Cambodia. Asian Perspectives 45: 188–211.\nRautman A. E. (Ed.). 2000. Reading the body: Representations and remains in the archaeological record. Philadelphia: University of Pennsylvania Press.\nRedman SJ. 2016. Bone rooms: From scientific racism to human prehistory in museums. Harvard University Press: Cambridge, MA.\nReece A.G., Danforth C.M. 2017. Instagram photos reveal predictive markers of depression. EPJ Data Science 6: 15-21.\nSchmidhuber J. 2015. Deep learning in neural networks: An overview. Neural Networks 61: 85-117. Seidemann R.M., Stojanowski C.M. 2009. The identification of a skull recovered from an eBay sale. Journal of Forensic Sciences 54: 1247-1253.\nSmith IV J. 2014. Here\u0026rsquo;s every statistic you could want on Instagram drug dealers, http://observer.com/2014/05/heres-every-statistic-you-could-want-on-instagram-drug-dealers/.\nSpars, S. 2010. Understanding conflict through burial. Neural network analysis of death and burial in the war of 1812. Ontario Archaeology. 89\u0026frasl;90: 58-68.\nThosarat, R. 2001. The destruction of the cultural heritage of Thailand and Cambodia. Trade in Illicit Antiquities: the destruction of the world’s archaeological heritage 1:7-18.\nTotti L.C., Costa F.A., Avila S., Valle E., Meira Jr. W., Almeida V. 2014. The impact of visual attributes on online image diffusion. Proceedings of the 2014 ACM conference on Web science, pp. 42-51.\nTsou M-H, Leitner M. 2013. Visualization of social media: Seeing a mirage or a message? Cartography and Geographic Information Science 40: 55-60.\nUshizima D, Manovich L, Margolis T, Douglass J. 2012. Cultural analytics of large datasets from Flickr. Workshop on Social Media Visualization, Dublin, Ireland, May 20, 2012.\nVergano D. 2016. eBay just nixed its human skull market. https://www.buzzfeed.com/danvergano/skull- sales?utm_term=.gtOnv2yz4#.fbOdBGMzm.\nWang, H. He, Z. Huang, Y., Chen, D., Zhou, Z. 2017. Bodhisattva head images modeling style recognition of Dazu Rock Carvings based on deep convolutional network. Journal of Cultural Heritage 27: 60-71.\nYang X., Luo J. 2017. Tracking illicit drug dealing and abuse on Instagram using multimodal analysis. ACM Transactions on Intelligent Systems and Technology (TIST) - Special Issue: Cyber Security and Regular Papers vol. 8, article no. 58.\nYates D., Mackenzie S., Smith E. 2017. The cultural capitalists: Notes on the ongoing reconfiguration of trafficking culture in Asia. Crime, Media, Culture: An International Journal 13: 245-254.\nYucha JM, Pokines JT, Bartelink EJ. 2017. A comparative taphonomic analysis of 24 trophy skulls from modern forensic cases. Journal of Forensic Sciences 62: 1266-1278.\nZhou X., Yu K., Zhang T., Huang T.S. 2010. Image classificaiton using super-vector coding of local image descriptors. In: Daniilidis K., Maragos P., Paragios N. (eds.) Computer Vision – ECCV 2010. ECCV 2010. Lecture Notes in Computer Science, vol. 6315. Springer, Berlin, Heidelberg.\n"
},
{
	"uri": "/scope/outputs/",
	"title": "Expectations for What We&#39;ll Learn",
	"tags": [],
	"description": "Project outputs",
	"content": "We expect to learn\n the strengths and limitations of neural networks as a lens for studying the trade in human remains - a much finer-grained picture of the scope and size of the trade in human remains the identification of flows of visual signifiers of what is for sale, where it comes from, and where the tastes in this market are set the identification of institutions, collections, cultures, or conflicts from which human skeletal remains actively being circulated are sourced how to deploy neural networks in the service of humanistic and social science inquiry  We expect to provide\n high quality training to exceptional students who will use this training to launch their careers anonymized and ethical exposure of the trade to both the public and to policy makers  As outputs we will have created\n several articles in scholarly and popular journals fully cleaned \u0026amp; anonymized datasets for publication and reuse tutorials for use outside our institutions seven highly trained personnel new course opportunities for undergraduates training and research relationships between Carleton University and the University of Stockholm a new direction for the study of the trade in illicit and illegal antiquities more generally  At the conclusion of this proposed project, we believe that we will\n be in a position to begin another phase of this research where we will take our results and our method, where digital cultural heritage ethics are foregrounded. We hope to be able to support descendent communities at risk, by providing training and support in computational methods.\n be in a position to influence change at social media companies that currently permit the trade in human remains have raised the public consciousness of this trade, including with law-enforcement agencies, policy makers, and the corporations whose platforms currently facilitate the trade  "
},
{
	"uri": "/scope/graduate-opportunities/",
	"title": "Graduate Opportunities",
	"tags": [],
	"description": "",
	"content": " We currently have funded opportunities for PhD and MA students; see below.\nGraduate students in History/Digital Humanities and History/Data Science will be trained in reproducible computational approaches and open science, as well as the necessary grounding in the antiquities trade, and the human remains subtrade. Digital training will include everything from version control to writing analytical packages, to interactive website design for tutorial-writing and outreach. They will learn state of the art machine learning and neural network approaches with social media data.\nStudents will be involved in the outset in planning our research design; they will have opportunities to publish as lead authors (especially with regard to mobilizing knowledge through venues such as The Programming Historian and conferences such as DHSITES). MA students will have the opportunity to work in a leadership role with students in Graham’s undergraduate digital history research methods courses (HIST3812 and HIST3814); the PhD student will have the opportunity to design and teach undergraduate courses on the intersection of data ethics and history/archaeology. The graduate students will be encouraged and supported to hold ‘unconferences’ (see thatcamp.org) on a theme of their choosing; an excellent opportunity exists for international participation and recognition of their work should they choose to hold one (as we will encourage them to do) during the DH2020 Conference in Ottawa.\nTheir research may include:\n Writing code to generate datasets Developing various NN Analyzing results Ground-truthing training datasets (making sure that training images are properly classified) Curating and preparing materials for data publication in appropriate venues Research and writing of tutorials Research and writing connected with their own research interests as they intersect with this project Communicating the results of research with relevant publics at conferences and other venues  Funded PhD Opportunity\n\u0026lsquo;Ethics, Digital Humanities, and the Computer Gaze\u0026rsquo; We are seeking an individual to explore from an ethical perspective for public history and cultural heritage writ large, the use of various digital technologies broadly classified as \u0026lsquo;AI\u0026rsquo; on photographic (or other social media) materials. This research would form an integral part of Shawn Graham and Damien Huffer\u0026rsquo;s \u0026ldquo;The Bone Trade: Studying the Online Trade in Human Remains with Machine Learning and Neural Networks Project\u0026rdquo;. The student would start ideally in September 2018.\nThe candidate\u0026rsquo;s major field would be in Public History, with a breadth field in Digital Humanities\nInterested candidates are invited to contact Dr. Shawn Graham to discuss their potential research project, to gauge their potential fit with the funding envelope and other potential supplementary funding sources. Candidates are also invited to review the Phd Program requirements at https://carleton.ca/history/graduate/phd-program/program-requirements/\nInternational students are also invited to apply.\nCriteria:\n a good MA degree in a relevant subject (history, archaeology, etc) some existing ability in digital humanities methods is desirable  Funded MA Opportunities History, Public History, and Digital Humanities We are seeking two potential MA students (to start September 2018) to collaborate on \u0026ldquo;The Bone Trade: Studying the Online Trade in Human Remains with Machine Learning and Neural Networks Project\u0026rdquo;. The students would pursue their own research within the ambit of this project, which revolves around the use of various AI technologies investigating the trade in human remains online. Ideally, the students\u0026rsquo; own research projects would push the research into other domains, for instance, historical photographs; tourist photos; advertising using historical imagery, digital historical consciousness.\nInterested candidates are invited to contact Dr. Shawn Graham to discuss their potential research project, to gauge their potential fit with the funding envelope and other potential supplementary funding sources. Candidates are also invited to review the MA History, MA Public History, with Collaborative Digital Humanities Program requirements -\nhttps://carleton.ca/history/graduate/ma-program/program-requirements/m-a-with-specialization-in-digital-humanities/\nhttps://carleton.ca/history/m-a-in-public-history/\nCriteria: - a good first degree in a relevant subject (history, archaeology, etc) - existing ability in digital humanities methods or issues is desirable, but not critical. Much more important is an ability to think creatively about the problems or potentials of computational viewpoints.\n"
},
{
	"uri": "/tutorials/scrapy-instagram/",
	"title": "Scraping Instagram Post Metadata",
	"tags": [],
	"description": "A short tutorial on using Scrapy to scrape Instagram",
	"content": " Instagram used to have an API that allowed a very comprehensive exploration of the metadata attached to each . To use this API, one had to register as a developer and agree to the terms of service. Pablo Barbera from the London School of Economics has developed an excellent package for R, instaR to work with the API. However, changes to the API and the approval process in the last few years has restricted use of the public API to mostly commercial purposes. That is to say, they wish to monetize the content. The result is that using Barbera\u0026rsquo;s package is often not feasible for us.\nOther alternatives are possible, and while they do not necessarily provide complete access to the full stream of possible data, they do provide an awful lot. In essence, these approaches involve mimicing an individual paging through search results, very very quickly.\nHere we will get started with a python package written by Github user h4t0n, Andrea Tarquini that depends on the Scrapy framework.\nRequirements:  Python Scrapy  If you are on a mac, you already have python installed. We will install Scrapy inside of a virtual environment, so as to avoid making dependency conflicts and so on with other python packages on our machine.\n$ mkdir scrapytest $ virtualenv scrapytest $ source scrapytest/bin/activate  Install Scrapy: $ pip install scrapy\nDownload the instagram-scraper from the repository.\nUnzip that file. Then, make a new text file at the same level as the instagram-scraper folder. Give that file the extension .cfg\n├── instagram-scraper │ ├── __init__.py │ ├── items.py │ ├── pipelines.py │ ├── settings.py │ └── spiders │ ├── __init__.py │ └── hashtag.py └── scrapy.cfg  Edit that file so that it contains the basic configuration information scrapy needs in order to pull together the scraper that h4t0n made:\n# basic information for scrapy cfg file [settings] default = instagram-scraper.settings [deploy] #url = http://localhost:6800/ project = tutorial  The first thing this file does is tell scrapy where to find the settings.py file in the instagram-scraper folder. The second line isn\u0026rsquo;t even really needed, but these are the defaults when creating a new scrapy scraper, so we\u0026rsquo;ll keep them for posterity.\nRunning the scraper If you look in the instagram-scraper folder, you\u0026rsquo;ll notice the spiders subfolder. The hashtag.py file is the one that lays out how to interact with Instagram. We want Scrapy to use that file, so:\n$ scrapy crawl hashtag\nYou will then be prompted to enter the target hashtag. The spider will then begin searching and scraping. Output prints to the terminal window and to file.\nOutput The output will be in a new folder, \u0026lsquo;scraped\u0026rsquo; and subfolder, \u0026lsquo;hashtag\u0026rsquo;. Each hashtag search will have its own subfolder, and the output file there will have its timestamp as its name. Each line in the output is json formatted. You can concatenate all of the files together with cat * \u0026gt; output.json. To make a properly formatted json file, open output.json in a text editor, add , to each line (except the last one), and put a [ at the start of the file and a ] at the end.\nThe data is now ready to be put into a database for further querying.\nDeactivate your virtualenv:\n$ deactivate\n"
},
{
	"uri": "/tutorials/sqlite/",
	"title": "Setting up a SQLite DB",
	"tags": [],
	"description": "A short tutorial on setting up a sqlite db and simple queries of same",
	"content": " The data that we scraped using Scrapy was output as json files. Json stands for \u0026lsquo;JavaScript Object Notation\u0026rsquo; and records values as attribute-data pairs and lists these pairs in arrays. Because we are not building a complex multi-faceted database, and we might want to expose this data for other kinds of uses later on (perhaps as our own API), we are going to take that data and put it into a sqlite database.\nThis will also enable us to identify duplicate values (data that got captured via multiple searches or hashtags, for instance), extract and reshape the data for particular purposes, connect our data to R for subsequent analyses, and deposit the data in our data repo.\nRequirements  sqlitebiter a python utility for ingesting a wide variety of data formats into an sqlite db python  First, set up a virtualenv:\n$ mkdir database-work $ virtualenv database-work $ source database-work/bin/activate  Install sqlitebiter:\n$ pip install sqlitebiter  This will install a bunch of other dependencies - which is why we\u0026rsquo;re using a virtualenv, so that these dependencies don\u0026rsquo;t conflict with other packages we\u0026rsquo;re using.\nMaking the database Assuming you have the concatenated file containing all of the results from the scrape as a properly formatted json file in the current folder,\nsqlitebiter file scrape-results.json -o scrape-results-current-date.sqlite  Then, to finish things up,\n$ sqlite3 scrape-results-current-date.sqlite \u0026gt;\u0026gt;\u0026gt; .schema \u0026gt;\u0026gt;\u0026gt; .exit  The \u0026gt;\u0026gt;\u0026gt; on the command line show us we\u0026rsquo;re inside sqlite. We could continue to work on the command line with sqlite, but sometimes a gui does make life easier. We\u0026rsquo;ll use sqlitebrowser. Download and install. Then, open the dbbrowser and open the database you just created.\nClick \u0026lsquo;browse data\u0026rsquo; and you\u0026rsquo;ll see the data in a clean table view. The data can be exported as csv. Here are some simple SQL queries we can run to do some common tasks for us. To run these, click \u0026lsquo;execute sql\u0026rsquo; and paste the query into the edit window. Once the query is there (these queries can also be saved and loaded from file) hit the \u0026lsquo;run\u0026rsquo; button to execute.\nExample Queries A query to select distinct rows, that is to say, remove duplicate values:\nselect distinct id, caption, display_url, owner_name, taken_at_timestamp from scrape-results-current-date;  make a new table from those distint values:\ncreate table individual_posts as select distinct id, caption, display_url, owner_name, taken_at_timestamp from scrape-results-current-date;  select distinct rows by keyword in a field:\nselect distinct id, caption, display_url, owner_name, taken_at_timestamp from individual_posts where caption like '%sale%';  convert unix timestamp to human-readable date\nUPDATE unique_posts SET taken_at_timestamp = datetime(taken_at_timestamp, 'unixepoch', 'localtime')  "
},
{
	"uri": "/tutorials/sqlite-to-r/",
	"title": "Importing SQLite DB into R",
	"tags": [],
	"description": "A workflow for importing DB into R",
	"content": " I\u0026rsquo;ve got data, now what? We\u0026rsquo;ve already explained how to put data into a SQLite database. Now we want to grab a subset of that data and pull it into R where we can analayze or visualize it, or combine it with other kinds of data. It is in fact quite straightforward.\nWe write a script that tells R to open a connection to the database; then we tell it what\u0026rsquo;s inside that database; then we write an SQL query in R that grabs just the bits we\u0026rsquo;re interested in and passes it to a new R object; and then we shut the connection and clear the cache.\nIn this way, we\u0026rsquo;re never directly modifying our datastore. We\u0026rsquo;re always working on a copy of the data, which is handy because when we screw up - and we will - we can rest easy knowing the original data is still ok. The code looks like this:\n# first we get the packages we need: library(DBI) library(RSQLite) # now we open the connection: con = dbConnect(SQLite(), dbname=\u0026quot;YOUR-AMAZING-DATABASE.sqlite\u0026quot;) # we can see what's inside, eg, what tables are in the database? alltables = dbListTables(con) alltables # write the query to get the information you want myQuery \u0026lt;- dbSendQuery(con, \u0026quot;SELECT id, owner_id, caption, taken_at_timestamp FROM unique_posts\u0026quot;) # pass that information to an R object. The n = -1 bit means grab everything until there's nothing left to grab. Otherwise, you can specify how many rows etc. my_data \u0026lt;- dbFetch(myQuery, n = -1) # now that we're done, clear cache dbClearResult(myQuery) # now carry on and begin manipulating my_data # for more information see # http://tiffanytimbers.com/querying-sqlite-databases-from-r/ # also perhaps this https://www.r-bloggers.com/using-sqlite-in-r/  "
},
{
	"uri": "/tutorials/classify-images-with-tensorflow/",
	"title": "Classify Images with Tensorflow",
	"tags": [],
	"description": "",
	"content": "In our 2018 paper, \u0026lsquo;Fleshing out the Bones\u0026rsquo;, we used Duhaime\u0026rsquo;s modified classify_images.py script using Tensorflow and Inception 3 to extract the image vectors from the second-to-last layer of the cnn (before labelling). These image vectors can then be fed through cluster_vectors.py to find nearest neighbors. This data can be visualized with t-sne; then we use affinity propagation to see the clusters in R.\nYou may obtain and run the code for yourself in our online jupyter notebook binder.\nThe repository for this data is at https://osf.io/9cfja/.\n"
},
{
	"uri": "/scope/end-of-project/",
	"title": "When This Project Ends",
	"tags": [],
	"description": "How this site and project data shall be archived",
	"content": "When this project reaches its conclusion, this website will no longer be updated. It will remain online here on Github for as long as Github allows us to host it here. We will also feed the url into the Internet Archive periodically to enable snapshots of the site and project there. We will create a web archive file for the site (for more on web archives, see the Web Archive Research Group).\nOur code, cleaned/anonymized data, articles, and the materials on this site will be deposited in Zenodo.org with appropriate DOIs.\nEach journal article will have its own research compendium attached, to promote replicability and reproducibility of our research (see Marwick et al 2017, \u0026lsquo;Open Science and Archaeology\u0026rsquo; SAA Archaeological Record)\nOur shared Zotero library may be viewed at https://www.zotero.org/groups/2174147/bonetrade-project.\n"
},
{
	"uri": "/update/2018-04-30-using-etudier-to-jump-start-a-literature-review/",
	"title": "Using Etudier to Jump Start a Literature Review",
	"tags": [],
	"description": "A workflow for starting that literature review",
	"content": " 2018-04-30\nThe Literature Review How do you start a literature review on a new project? We\u0026rsquo;re not starting from scratch, of course - we\u0026rsquo;ve got the pages of bibliography that we generated when we first starting scratching out what this project could be about. We started that step by considering the bibliography we had to hand as we wrote our first paper. Nothing is ever truly from scratch, in academia.\nBut we can certainly jump start the process. One way we can do that is by pulling from one of the largest bibliographic databases readily to hand - Google Scholar. G-Scholar was built by ingesting several existing databases and is maintained by whatever magic Google uses to index journal websites and so on. We can search this database not only how we might search with the regular Google search page, but also via the citation graph. We can give it a reference, and search not only what that paper cites, but also the papers that cited it in turn. What\u0026rsquo;s more, we can automate this process.\nEd Summers has created a python package called \u0026lsquo;Etudier\u0026rsquo; which handles this for us. It mimics a user paging through search results. By default it only looks at the first ten citations for a page or a search result, and then the top 10 for each of those, thus 100 results. That might be enough to get going. But by fiddling with the --pages and --depth flags, you can collect much much more. pages will search through x number of pages of results, while depth will delve that far into each result\u0026rsquo;s citations.\nI ran the following:\n$etudier.py 'https://scholar.google.com/scholar?hl=en\u0026amp;as_sdt=0%2C21\u0026amp;q=\u0026quot;instagram\u0026quot;\u0026amp;btnG=' --pages 2 --depth 2  \u0026hellip;to see what the broader landscape of scholarship around Instagram looked like. This produced 1.4 mb of results, or 1883 articles tied by 2179 edges.\nAs a graph, we spot something interesting right away:\nTwo broad clumps, or two extremely distinct ways of referring to \u0026lsquo;Instagram\u0026rsquo;. We can then explore subclumping by searching for communities or patterns of link similarities (aka as \u0026lsquo;modularity\u0026rsquo;):\nOften however this kind of visualization is not of much use, other than how we\u0026rsquo;ve already used it. Instead, let\u0026rsquo;s look for centrality in this graph, and identify those works whose pattern of linkages enable them to bridge these various subgroups. I presume that such works have something about them that speaks to these different aspects of scholarship and so are the works that I\u0026rsquo;ll want to start with for my review. Using gephi I calculate eigenvector centrality (roughly, the centrality that comes from being wellconnected wellconnected others).\nYou can download our graph of data for yourself here.\n"
},
{
	"uri": "/update/2018-04-16-getting-started/",
	"title": "Getting Started!",
	"tags": [],
	"description": "getting started",
	"content": " 2018-04-16\nWe\u0026rsquo;re underway! We received word at the end of March that this project has been funded. Since then, we\u0026rsquo;ve been doing the administrative work to get the project up and running - figuring out how to advertise for students, how to enable those students to study at Stockholm for a term or two, setting up the public face of this project, figuring out the internals of the research accounting system.\nWe also took advantage of the experience of writing this proposal to run a small experiment in using Tensorflow on our previous corpus (Huffer \u0026amp; Graham 2017) of Instagram materials. We wrote that up for the Journal of Computer Applications in Archaeology and submitted it around the same time as we submitted the proposal. This was accepted with revisions, and is now in the pipeline. We hope to see it come out shortly. That paper was also the nucleus for a paper at the Society for American Archaeology conference this April in Washington DC. We also presented a related paper, drawing on Damien\u0026rsquo;s postdoctoral work- Huffer and Graham, Bioarchaeological Approaches to Investigating Supply, Demand and Authenticity in the Colonial-era Human Remains Trade.\nI guess this all means: we\u0026rsquo;re hitting the ground running!\n"
},
{
	"uri": "/",
	"title": "The Bonetrade: Studying the Online Trade in Human Remains",
	"tags": [],
	"description": "The online trade in human remains is little explored. We use neural network approaches to try to understand the visual and textual rhetorics that underpin it.",
	"content": " The Bonetrade: Studying the Online Trade in Human Remains with Machine Learning and Neural Networks There is a thriving online trade in anatomical, ethnographic and archaeological human remains that makes ready use of new social media such as Instagram, Facebook, Etsy, and until recently, eBay. The \u0026ldquo;fetishization\u0026rdquo; of the \u0026lsquo;exotic\u0026rsquo; dead that underpins this trade by its very nature transforms pieces of the body into material culture: curios, commodities or objets d\u0026rsquo;art. This practice has deep Colonial-era roots, but today\u0026rsquo;s e-commerce and social media platforms have only expanded collectors\u0026rsquo; reach and made participation open to anyone with interest and spare finances. The sheer volume of materials being produced, shared, and sold can be overwhelming for a small team to study. The market moves so fast.\nCan we teach machines to identify from photographs alone patterns in the \u0026lsquo;visual rhetoric\u0026rsquo; that signal materials for sale? Can \u0026lsquo;licit\u0026rsquo; materials be discerned from \u0026lsquo;illicit\u0026rsquo;? Are there geographical patterns? Can we trace materials back to a source?\nThis website is the public face of our research. Here you will find updates, code, research compendia, papers, presentations, and other elements of our work over the next five years. We intend to produce a body of data and of code that will enable other researchers to repurpose our research into other allied fields (such as the more familiar trade in archaeological antiquities, and in the markets for other \u0026lsquo;grey market\u0026rsquo; and black market commodities).\nOpen Research All our code and data relating to this project are publicly available. All our publications and reports are accompanied by a compendium of code and data files that are deposited in a trustworthy data repository and referenced via DOI. We use GitHub to publicly host our code in development, at github.com/bonetrade. We archive code and data files at the Open Science Foundation.\n"
},
{
	"uri": "/_footer/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "This research was supported by the Social Sciences and Humanities Research Council of Canada\n"
},
{
	"uri": "/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/credits/",
	"title": "Credits",
	"tags": [],
	"description": "contributors and packages used by hugo-theme-docdock",
	"content": " Principle Investigators Shawn Graham Graham is an archaeologist who is currently Associate Professor of Digital Humanities in the History Department at Carleton University. With Ian Milligan and Scott Weingart, he is the author of ‘Exploring Historical Big Data: The \u0026lsquo;Historian’s Macroscope’ (2015). He has written a number of tutorials for The Programming Historian and his research blog (e.g., 2016b) on different computational techniques for both analysis and representation of historical data. He is founder and editor of Epoiesen: A Journal for Creative Engagement in History and Archaeology and is currently building the Open Digital Archaeology Textbook (or ODATE), which will serve as a living resource of how to do digital archaeology in a way that “encourages innovative, reflective, and critical use of open access data and the development of digital tools that facilitate linkages and analysis across varied digital sources”.\nDamien Huffer Huffer is a human bioarchaeologist and osteologist by training. His postdoctoral work incorporates the collection of photographic and osteological data from ‘trophy skull’ assemblages (primarily Bornean Dayak and W. Papuan peoples) held in Swedish, US, and UK museums. He is a recognized authority on the osteological aspects of the trade in human remains.\nGraduate Students This project will feature prominently the work of our graduate students.\nUndergraduate Students Undergraduate students will also have the opportunity to work on aspects of this project as part of their course work, and will be recognized here.\nColophon This website is built using the docdocks theme for Hugo and is powered by Yihui Xie\u0026rsquo;s blogdown environment in R Studio\n"
},
{
	"uri": "/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/update/",
	"title": "Updates",
	"tags": [],
	"description": "",
	"content": ""
}]